# =============================================================================
# smartedu_fetch_all.sh  â€”â€”  Shell + Python polyglot (v1.0)
# -----------------------------------------------------------------------------
# æ–°å¢ï¼š
#  â€¢ æ•´è¼ªçµæŸå¾Œè‡ªå‹•é‡è©¦ï¼ˆé è¨­ 2 è¼ªï¼Œå¯ç”¨ -T N èª¿æ•´ï¼›0 è¡¨ç¤ºé—œé–‰ï¼‰ã€‚
#  â€¢ æ¯è¼ªé‡è©¦åƒ…é‡å°ä¸Šè¼ªå¤±æ•—æ¸…å–®ï¼ŒæˆåŠŸå³å¯«å› index.jsonï¼Œä»å¤±æ•—ä¿ç•™åˆ° failed.jsonã€‚
#  â€¢ æœ€çµ‚è‹¥ä»æœ‰å¤±æ•—ï¼Œæ¸…æ™°æç¤ºç”¨æˆ¶å¯å†æ¬¡é‹è¡Œæˆ–ç”¨ -R åƒ…é‡è©¦å¤±æ•—ã€‚
# å…¶ä»–ï¼š
#  â€¢ ä¿æŒ v4.2 çš„ç©©å®šæ€§ï¼šå¤šä¸»æ©Ÿç´¢å¼•/è©³æƒ…ã€Referer å®Œæ•´ã€HEAD/Range æ¢æ¸¬ã€æ–·é»çºŒå‚³ã€å»é‡ã€è©³ç›¡æ—¥èªŒã€‚
# -----------------------------------------------------------------------------
# ç”¨æ³•ï¼š
#   bash smartedu_fetch_all.sh -p é«˜ä¸­
#   bash smartedu_fetch_all.sh -p é«˜ä¸­ -s è¯­æ–‡,æ•°å­¦ -m "å¿…ä¿® ç¬¬ä¸€å†Œ" -T 3
#   bash smartedu_fetch_all.sh -R -o ./output_dir
# =============================================================================

# ---- å¦‚æœä»¥ python æ–¹å¼èª¿ç”¨ï¼Œç›´æ¥è·³é Shell éƒ¨åˆ† ----
if [ -n "${PYTHON_EXEC:-}" ]; then
  :
else
  set -euo pipefail

  # èª¿è©¦æ¨¡å¼ï¼šDEBUG=1 æ™‚æ‰“å°åŸ·è¡Œç´°ç¯€
  if [ "${DEBUG:-0}" = "1" ]; then set -x; fi

  # è¨˜éŒ„ç•¶å‰å·¥ä½œç›®éŒ„ï¼Œé¿å… /dev/fd è·¯å¾‘é€ æˆç›¸å°è·¯å¾‘æ··äº‚
  PWD_ABS="$(pwd)"

  # å°æ–¼ apt ç³»çµ±ï¼Œé è¨­ç‚ºéäº’å‹•æ¨¡å¼ï¼Œé¿å…å®‰è£ä¸­é€”åœä¸‹
  if command -v apt-get >/dev/null 2>&1 || command -v apt >/dev/null 2>&1; then
    export DEBIAN_FRONTEND=${DEBIAN_FRONTEND:-noninteractive}
  fi

  # é è¨­åƒæ•¸
  PHASE="é«˜ä¸­"
  SUBJECTS="è¯­æ–‡,æ•°å­¦,è‹±è¯­,æ€æƒ³æ”¿æ²»,å†å²,åœ°ç†,ç‰©ç†,åŒ–å­¦,ç”Ÿç‰©"
  FORCE_OVERWRITE="0"
  CATALOG_ONLY="0"
  WORKER_URL=""
  HCON=12
  DCON=5
  POST_RETRY=2
  
  # ---- äº¤äº’å¼é…ç½®ï¼ˆå§‹çµ‚é–‹å•Ÿï¼‰ ----
  if [ -t 0 ]; then
    printf "\n================ ğŸ“š æ™ºæ…§æ•™è‚²æ•™æä¸‹è¼‰å™¨ ================\n"
    printf "æ­¡è¿ä½¿ç”¨ï¼æœ¬å·¥å…·å°‡å¹«åŠ©æ‚¨ä¸‹è¼‰åœ‹å®¶æ•™ææˆ–ç”Ÿæˆç›®éŒ„ã€‚\n\n"

    # 1. æ¨¡å¼é¸æ“‡
    printf "ğŸ‘‰ [1/4] è«‹å•æ‚¨æƒ³åšä»€éº¼ï¼Ÿ\n"
    printf "   1) ä¸‹è¼‰æ•™æ PDF åˆ°æœ¬åœ°ï¼ˆé»˜èªï¼Œé©åˆæ‰“å°æˆ–é›¢ç·šé–±è®€ï¼‰\n"
    printf "   2) åƒ…ç”Ÿæˆç¶²ç«™ç›®éŒ„ï¼ˆä¸ä¸‹è¼‰ PDFï¼Œç”Ÿæˆä¸€å€‹ç¶²é ç‰ˆç›®éŒ„ï¼‰\n"
    read -r -p "è«‹è¼¸å…¥æ•¸å­— [1-2] (é»˜èª 1): " ans
    if [ "$ans" = "2" ]; then
        CATALOG_ONLY="1"
        printf "\n   [i] å·²é¸æ“‡ã€Œåƒ…ç›®éŒ„æ¨¡å¼ã€ã€‚å°‡ç”ŸæˆåŒ…å«ä¸‹è¼‰éˆæ¥çš„ç¶²é ã€‚\n"
        if [ -z "$WORKER_URL" ]; then
             printf "   [?] è«‹è¼¸å…¥ Cloudflare Worker ä»£ç†åœ°å€ (å¯é¸ï¼Œé˜²æ­¢ 403 éŒ¯èª¤)\n"
             printf "       (å¦‚æœæ²’æœ‰ï¼Œå¯ç›´æ¥å›è»Šï¼Œä½†ç›´æ¥éˆæ¥å¯èƒ½å¤±æ•ˆ)\n"
             read -r -p "       Worker URL: " w_ans
             [ -n "$w_ans" ] && WORKER_URL="$w_ans"
        fi
    else
        CATALOG_ONLY="0"
    fi

    # 2. æ•™è‚²éšæ®µ
    printf "\nğŸ‘‰ [2/4] é¸æ“‡æ•™è‚²éšæ®µï¼š\n"
    printf "   1) å°å­¦    2) åˆä¸­    3) é«˜ä¸­ (é»˜èª)    4) ç‰¹æ®Šæ•™è‚²    5) å°å­¦54    6) åˆä¸­54\n"
    read -r -p "è«‹è¼¸å…¥æ•¸å­— [1-6]: " ans
    case "$ans" in
      1) PHASE="å°å­¦";;
      2) PHASE="åˆä¸­";;
      3) PHASE="é«˜ä¸­";;
      4) PHASE="ç‰¹æ®Šæ•™è‚²";;
      5) PHASE="å°å­¦54";;
      6) PHASE="åˆä¸­54";;
      *) [ -z "$PHASE" ] && PHASE="é«˜ä¸­";;
    esac
    printf "   [i] å·²é¸æ“‡: %s\n" "$PHASE"

    # 3. å­¸ç§‘é¸æ“‡ (Simplified menu)
    printf "\nğŸ‘‰ [3/4] é¸æ“‡å­¸ç§‘ï¼š\n"
    printf "   0) å…¨éƒ¨ä¸‹è¼‰ (é»˜èª)\n"
    
    # Common subjects list
    menu_subjs=("è¯­æ–‡" "æ•°å­¦" "è‹±è¯­" "ç‰©ç†" "åŒ–å­¦" "ç”Ÿç‰©" "å†å²" "åœ°ç†" "æ€æƒ³æ”¿æ²»" "ç§‘å­¦" "é“å¾·ä¸æ³•æ²»" "ä¿¡æ¯æŠ€æœ¯" "ä½“è‚²" "éŸ³ä¹" "ç¾æœ¯")
    i=1
    for s in "${menu_subjs[@]}"; do
        printf "   %2d) %-10s" "$i" "$s"
        if [ $((i % 4)) -eq 0 ]; then echo ""; fi
        i=$((i+1))
    done
    echo ""
    printf "   Tip: å¯è¼¸å…¥å¤šå€‹æ•¸å­—(å¦‚ 1,2,3) æˆ–ç›´æ¥è¼¸å…¥å­¸ç§‘åç¨±\n"
    
    read -r -p "è«‹è¼¸å…¥ (é»˜èª 0): " ans
    if [ -n "$ans" ]; then
        if [ "$ans" = "0" ]; then
            # Keep default SUBJECTS but maybe expand it if user wants ALL?
            # Actually default SUBJECTS in env variable is quite limited. 
            # If user selects ALL (0), we should probably set it to a very broad list or special value.
            # For now, let's set it to the full menu list plus defaults to be safe.
            SUBJECTS="è¯­æ–‡,æ•°å­¦,è‹±è¯­,ç‰©ç†,åŒ–å­¦,ç”Ÿç‰©,å†å²,åœ°ç†,æ€æƒ³æ”¿æ²»,ç§‘å­¦,é“å¾·ä¸æ³•æ²»,ä¿¡æ¯æŠ€æœ¯,ä½“è‚²ä¸å¥åº·,éŸ³ä¹,ç¾æœ¯,è‰ºæœ¯,åŠ³åŠ¨,ç»¼åˆå®è·µæ´»åŠ¨"
        elif [[ "$ans" =~ ^[0-9,]+$ ]]; then
            # Parse numbers
            new_subjs=""
            IFS=',' read -ra ADDR <<< "$ans"
            for id in "${ADDR[@]}"; do
                idx=$((id-1))
                if [ $idx -ge 0 ] && [ $idx -lt ${#menu_subjs[@]} ]; then
                    if [ -z "$new_subjs" ]; then new_subjs="${menu_subjs[$idx]}"; else new_subjs="$new_subjs,${menu_subjs[$idx]}"; fi
                fi
            done
            [ -n "$new_subjs" ] && SUBJECTS="$new_subjs"
        else
            # Assume manual text input
            SUBJECTS="$ans"
        fi
    fi
    printf "   [i] å·²é¸æ“‡: %s\n" "$SUBJECTS"

    # 4. å¼·åˆ¶è¦†è“‹
    if [ "$CATALOG_ONLY" = "0" ]; then
        printf "\nğŸ‘‰ [4/4] æ˜¯å¦é‡æ–°ä¸‹è¼‰å·²å­˜åœ¨ä¸”å®Œæ•´çš„æ–‡ä»¶ï¼Ÿ\n"
        read -r -p "è¼¸å…¥ y é‡æ–°ä¸‹è¼‰ï¼Œç›´æ¥å›è»Šè·³é (é»˜èªè·³é): " ans
        if [ "$ans" = "y" ] || [ "$ans" = "Y" ]; then
             FORCE_OVERWRITE="1"
        fi
    fi

    printf "\nâœ… é…ç½®å®Œæˆï¼å³å°‡é–‹å§‹ä»»å‹™...\n"
    printf "==============================================\n\n"
    sleep 1
  fi

  # è‹¥ä½¿ç”¨å¼·åˆ¶æ¨¡å¼ï¼Œè©¢å•æ˜¯å¦æ¸…é™¤èˆŠ PDF
  if [ "$FORCE_OVERWRITE" = "1" ] && [ -t 0 ]; then
    # æª¢æŸ¥è¼¸å‡ºç›®éŒ„æ˜¯å¦å­˜åœ¨ PDF æ–‡ä»¶
    _CHECK_DIR="./smartedu_textbooks"
    
    if [ -d "$_CHECK_DIR" ]; then
      _PDF_COUNT=$(find "$_CHECK_DIR" -name "*.pdf" -type f 2>/dev/null | wc -l | tr -d ' ')
      if [ "$_PDF_COUNT" -gt 0 ]; then
        printf "\nâš ï¸  ç™¼ç¾ %s å€‹ç¾æœ‰ PDF æ–‡ä»¶åœ¨ %s\n" "$_PDF_COUNT" "$_CHECK_DIR"
        printf "    å¼·åˆ¶æ¨¡å¼æœƒé‡æ–°ä¸‹è¼‰æ‰€æœ‰æ–‡ä»¶ï¼Œä½†ä¸æœƒè‡ªå‹•åˆªé™¤èˆŠæ–‡ä»¶ã€‚\n"
        read -r -p "æ˜¯å¦åœ¨é–‹å§‹å‰æ¸…é™¤æ‰€æœ‰èˆŠ PDFï¼Ÿ(y/N): " ans
        if [ "$ans" = "y" ] || [ "$ans" = "Y" ]; then
          printf "[*] æ­£åœ¨æ¸…é™¤èˆŠ PDF æ–‡ä»¶..."
          find "$_CHECK_DIR" -name "*.pdf" -type f -delete 2>/dev/null
          find "$_CHECK_DIR" -name "*.part" -type f -delete 2>/dev/null
          printf " å®Œæˆ\n"
        else
          printf "[i] ä¿ç•™èˆŠæ–‡ä»¶ï¼Œæ–°ä¸‹è¼‰å°‡è¦†è“‹åŒåæ–‡ä»¶\n"
        fi
      fi
    fi
  fi

  # äº¤äº’è¼¸å…¥å¾Œå†åšä¸€æ¬¡æ•¸å€¼æ ¡é©—
  int_re='^[0-9]+$'
  # äº¤äº’è¼¸å…¥å¾Œå†åšä¸€æ¬¡æ•¸å€¼æ ¡é©— (Optional, kept for safety)
  int_re='^[0-9]+$'
  if ! [[ "$HCON" =~ $int_re ]]; then HCON=12; fi
  if ! [[ "$DCON" =~ $int_re ]]; then DCON=5; fi

  # --- æ¬Šé™èˆ‡åŒ…ç®¡ç†å™¨åµæ¸¬ ---
  if [ "${EUID:-$(id -u)}" -ne 0 ]; then SUDO="sudo"; else SUDO=""; fi
  have() { command -v "$1" >/dev/null 2>&1; }
  pm=""
  if have apt-get; then pm=apt; elif have apt; then pm=apt; elif have dnf; then pm=dnf; elif have yum; then pm=yum; elif have pacman; then pm=pacman; elif have zypper; then pm=zypper; elif have apk; then pm=apk; elif have brew; then pm=brew; fi

  # --- å®‰è£ Python èˆ‡ pip/venvï¼Œæ¶µè“‹ä¸»æµç™¼è¡Œç‰ˆ ---
  install_python() {
    echo "[*] æº–å‚™ Python ç’°å¢ƒ... (pkgmgr=$pm)"
    case "$pm" in
      apt)
        $SUDO apt-get update -y -qq || true
        if [ -n "$SUDO" ]; then
          $SUDO env DEBIAN_FRONTEND=${DEBIAN_FRONTEND:-noninteractive} \
            apt-get install -y -qq \
              -o Dpkg::Options::=--force-confdef \
              -o Dpkg::Options::=--force-confnew \
              python3 python3-venv python3-pip ca-certificates
        else
          env DEBIAN_FRONTEND=${DEBIAN_FRONTEND:-noninteractive} \
            apt-get install -y -qq \
              -o Dpkg::Options::=--force-confdef \
              -o Dpkg::Options::=--force-confnew \
              python3 python3-venv python3-pip ca-certificates
        fi
        ;;
      dnf)
        $SUDO dnf install -y python3 python3-pip
        ;;
      yum)
        $SUDO yum install -y python3 python3-pip
        ;;
      pacman)
        $SUDO pacman -Sy --noconfirm python python-pip
        ;;
      zypper)
        $SUDO zypper -n install python3 python3-pip
        ;;
      apk)
        $SUDO apk add --no-cache python3 py3-pip ca-certificates
        ;;
      brew)
        brew update >/dev/null || true
        brew install python || true
        ;;
      *)
        echo "[!] æœªè­˜åˆ¥çš„åŒ…ç®¡ç†å™¨ï¼Œè«‹æ‰‹å‹•å®‰è£ python3/pipã€‚" >&2
        ;;
    esac

    # è‹¥ç¼º ensurepipï¼Œå˜—è©¦ä¿®å¾©
    if ! python3 - <<'PY' 2>/dev/null
import ensurepip; print('ok')
PY
    then
      echo "[*] å˜—è©¦å•Ÿç”¨ ensurepip..."
      python3 -m ensurepip --upgrade >/dev/null 2>&1 || true
    fi

    # è‹¥ä»ç„¡ pipï¼Œä½¿ç”¨ get-pip å¼•å°
    if ! python3 -m pip --version >/dev/null 2>&1; then
      echo "[*] ä½¿ç”¨ get-pip å¼•å°å®‰è£ pip..."
      TMPPIP="$(mktemp -t getpip_XXXX).py"
      if have curl; then curl -fsSL https://bootstrap.pypa.io/get-pip.py -o "$TMPPIP"; elif have wget; then wget -qO "$TMPPIP" https://bootstrap.pypa.io/get-pip.py; else echo "[!] éœ€è¦ curl æˆ– wget ä¸‹è¼‰ get-pip.py" >&2; exit 1; fi
      python3 "$TMPPIP" >/dev/null
      rm -f "$TMPPIP"
    fi
  }

  if ! have python3; then
    if [ -z "$pm" ]; then echo "[!] æœªæª¢æ¸¬åˆ°åŒ…ç®¡ç†å™¨ä¸”ç³»çµ±ç„¡ python3ï¼Œè«‹å…ˆæ‰‹å‹•å®‰è£ã€‚" >&2; exit 1; fi
    install_python
  else
    # æŸäº› Debian/Ubuntu ç²¾ç°¡é¡åƒé›–æœ‰ python3 ä½†ç¼º venv æ¨¡å¡Š
    if [ "$pm" = apt ] && ! python3 -c 'import venv' 2>/dev/null; then
      echo "[*] å®‰è£ python3-venv ..."; $SUDO apt-get update -y -qq; \
      if [ -n "$SUDO" ]; then
        $SUDO env DEBIAN_FRONTEND=${DEBIAN_FRONTEND:-noninteractive} apt-get install -y -qq python3-venv
      else
        env DEBIAN_FRONTEND=${DEBIAN_FRONTEND:-noninteractive} apt-get install -y -qq python3-venv
      fi
    fi
    # è‹¥ç„¡ pip äº¦è£œé½Š
    if ! python3 -m pip --version >/dev/null 2>&1; then
      install_python
    fi
  fi

  # --- å»ºç«‹è™›æ“¬ç’°å¢ƒï¼ˆå¤±æ•—å‰‡ä¿®å¾©å¾Œé‡è©¦ï¼Œä»å¤±æ•— fallback ç³»çµ± Pythonï¼‰ ---
  # å…è¨±å¤–éƒ¨å¼·åˆ¶ä½¿ç”¨ç³»çµ± Pythonï¼šUSE_SYSTEM_PY=1 bash jks.sh ...
  if [ "${USE_SYSTEM_PY:-0}" = "1" ]; then
    echo "[i] å·²æŒ‡å®š USE_SYSTEM_PY=1ï¼Œè·³é venv æ§‹å»ºï¼Œç›´æ¥ä½¿ç”¨ç³»çµ± Pythonã€‚"
  fi

  VENV_DIR="${VENV_DIR:-$PWD_ABS/.venv}"
  if [ "${USE_SYSTEM_PY:-0}" != "1" ]; then
    if [ ! -d "$VENV_DIR" ]; then
      echo "[*] å‰µå»ºè™›æ“¬ç’°å¢ƒ $VENV_DIR"
      if ! python3 -m venv "$VENV_DIR" 2>/tmp/venv.err; then
        echo "[!] venv å»ºç«‹å¤±æ•—ï¼Œå˜—è©¦ä¿®å¾©..."
        if [ "$pm" = apt ]; then
          if [ -n "$SUDO" ]; then
            $SUDO env DEBIAN_FRONTEND=${DEBIAN_FRONTEND:-noninteractive} apt-get install -y -qq python3-venv || true
          else
            env DEBIAN_FRONTEND=${DEBIAN_FRONTEND:-noninteractive} apt-get install -y -qq python3-venv || true
          fi
        fi
        python3 -m ensurepip --upgrade >/dev/null 2>&1 || true
        if ! python3 -m venv "$VENV_DIR" 2>>/tmp/venv.err; then
          echo "[!] ä»ç„¡æ³•å»ºç«‹ venvï¼Œå°‡æ”¹ç”¨ç³»çµ± Python ç¹¼çºŒï¼ˆå»ºè­°ç¨å¾Œä¿®å¾© venvï¼‰ã€‚" >&2
          USE_SYSTEM_PY=1
        fi
      fi
    fi
  fi

  if [ "${USE_SYSTEM_PY:-0}" != "1" ]; then
    if [ -f "$VENV_DIR/bin/activate" ] && [ -x "$VENV_DIR/bin/python3" ]; then
      # shellcheck disable=SC1091
      . "$VENV_DIR/bin/activate"
      echo "[i] å·²å•Ÿç”¨è™›æ“¬ç’°å¢ƒï¼š$VENV_DIR"
    else
      echo "[!] venv æ§‹å»ºä¸å®Œæ•´ï¼Œæ‰¾ä¸åˆ° $VENV_DIR/bin/activate æˆ– python3ï¼›å°‡æ”¹ç”¨ç³»çµ± Python ç¹¼çºŒã€‚" >&2
      USE_SYSTEM_PY=1
      if [ -f /tmp/venv.err ]; then
        echo "[i] venv å»ºç«‹éŒ¯èª¤æ‘˜è¦ï¼š" >&2
        tail -n 50 /tmp/venv.err >&2 || true
      fi
      echo "[i] ç•¶å‰å·¥ä½œç›®éŒ„ï¼š$PWD_ABSï¼›VENV_DIR=$VENV_DIR"
      echo "[i] ç›®éŒ„åˆ—èˆ‰ï¼š"; ls -la "$PWD_ABS" || true
    fi
  fi

  # å®‰è£ä¾è³´
  echo "[i] ä½¿ç”¨çš„ Python: $(command -v python3)"
  python3 --version || true
  python3 -m pip install -U pip wheel setuptools >/dev/null
  python3 -m pip install -U aiohttp aiofiles tqdm >/dev/null

  export SMARTEDU_PHASE="$PHASE"
  export SMARTEDU_SUBJ="$SUBJECTS"
  # --- ç¢ºå®šè¼¸å‡ºç›®éŒ„ï¼ˆå›ºå®šç‚º ./smartedu_textbooksï¼‰ ---
  OUT_DIR="./smartedu_textbooks"
  mkdir -p "$OUT_DIR"
  export SMARTEDU_OUT_DIR="$OUT_DIR"
  echo "[i] ä¸‹è¼‰è¼¸å‡ºç›®éŒ„: $SMARTEDU_OUT_DIR"

  # ç¶²é ç”Ÿæˆå·²ç§»é™¤
  export SMARTEDU_HCON="$HCON"
  export SMARTEDU_DCON="$DCON"
  export SMARTEDU_POST_RETRY="$POST_RETRY"
  export SMARTEDU_FORCE="$FORCE_OVERWRITE"
  export SMARTEDU_CATALOG_ONLY="$CATALOG_ONLY"
  export SMARTEDU_WORKER_URL="$WORKER_URL"
  export PYTHON_EXEC=1

  # --- æ¸…ç†å­¤ç«‹çš„ .part æ–‡ä»¶ï¼ˆè¶…é 24 å°æ™‚ï¼‰ ---
  cleanup_stale_parts() {
    local dir="$1"
    if [ ! -d "$dir" ]; then return; fi
    local count=0
    while IFS= read -r -d '' f; do
      rm -f "$f" && count=$((count + 1))
    done < <(find "$dir" -name "*.part" -type f -mmin +1440 -print0 2>/dev/null)
    if [ "$count" -gt 0 ]; then
      echo "[i] å·²æ¸…ç† $count å€‹å­¤ç«‹çš„ .part æ–‡ä»¶"
    fi
  }
  cleanup_stale_parts "$OUT_DIR"

  # --- é…ç½® Nginx PDF è¨ªå•å°ˆç”¨æ—¥èªŒï¼ˆè‹¥ç³»çµ±æœ‰ nginxï¼‰ ---
  setup_nginx_pdf_logging() {
    if ! command -v nginx >/dev/null 2>&1; then return; fi
    local cfg="/etc/nginx/conf.d/textbook_pdf_logging.conf"
    if [ -f "$cfg" ]; then
      echo "[i] Nginx PDF logging å·²å­˜åœ¨: $cfg"; return;
    fi
    echo "[*] é…ç½® Nginx PDF å°ˆç”¨è¨ªå•æ—¥èªŒ..."
    $SUDO tee "$cfg" >/dev/null <<'NG'
# åœ¨ http å€å¡Šç”Ÿæ•ˆï¼šæŒ‰è«‹æ±‚ URI æ˜¯å¦ç‚º .pdf æ±ºå®šæ˜¯å¦è¨˜éŒ„
map $request_uri $is_textbook_pdf {
  default 0;
  ~*\.pdf$ 1;
}
log_format textbook '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" "$http_cf_connecting_ip" '
                    'host=$host uri=$request_uri bytes=$bytes_sent '
                    'sent_type=$sent_http_content_type';
access_log /var/log/nginx/textbook_access.log textbook if=$is_textbook_pdf;
NG
    $SUDO nginx -t && $SUDO systemctl reload nginx || echo "[!] Nginx é…ç½®æ¸¬è©¦/é‡è¼‰å¤±æ•—ï¼Œè«‹æ‰‹å‹•æª¢æŸ¥ã€‚"
  }
  setup_nginx_pdf_logging

  echo "[ğŸš€] å•Ÿå‹• Python ä¸‹è¼‰å™¨..."
  TMP_PY="$(mktemp)"
  awk '/^# >>>PYTHON>>>$/{p=1;next} /^# <<<PYTHON<<</{p=0} p' "$0" > "$TMP_PY"
  exec python3 "$TMP_PY"
  echo "[!] ç„¡æ³•å•Ÿå‹• Python å­é€²ç¨‹ï¼Œè«‹æª¢æŸ¥ä¸Šæ–¹æ—¥èªŒã€‚" >&2
  exit 1
fi

# >>>PYTHON>>>
# -*- coding: utf-8 -*-
"""
SmartEdu æ‰¹é‡ä¸‹è¼‰å™¨ (polyglot v5.0)
- å¾ã€Œä¸‹è¼‰ç’°ç¯€ã€å¾¹åº•å»é‡ï¼šè¦ç¯„å‘½åï¼ˆå»æ‰ __hash/_hash/-æ—¥æœŸ/æ™‚é–“æˆ³ å°¾ç¶´ï¼‰ï¼Œä¸‹è¼‰å‰åŸºæ–¼ Content-Length + ç¾æœ‰æ–‡ä»¶é€²è¡Œåˆ¤æ–·ï¼Œå·²å­˜åœ¨ä¸”æ›´å¤§/ç›¸ç­‰å‰‡è·³éã€‚
- æ–·é»çºŒå‚³ï¼š.part æª”è‡ªå‹•çºŒä¸‹ï¼›ä¸‹è¼‰å®Œæˆå¾ŒåŸå­æ›¿æ›ã€‚
- æˆåŠŸå¾Œå³åˆ»æ›´æ–° index.json èˆ‡ index.htmlï¼ˆæœ€å¾Œä¸€ç‰ˆé é¢æ¨£å¼ï¼‰ï¼Œå­¸ç§‘å°èˆªé»æ“Šå¦‚ã€Œèªæ–‡ã€æœƒåŒæ™‚é¡¯ç¤ºåˆä¸­/é«˜ä¸­ç­‰æ‰€æœ‰å­¸æ®µå·²ä¸‹è¼‰æ•™æã€‚
- çœŸæ­£å»é‡è¼¸å‡ºåˆ°ç¶²é ï¼šåŒå­¸ç§‘ + åŒã€Œè¦ç¯„æ›¸åã€åªé¡¯ç¤ºä¸€æ¢ï¼Œä¿ç•™é«”ç©æ›´å¤§çš„ç‰ˆæœ¬ã€‚
- è‡ªå‹•é‡è©¦è¼ªï¼šæ•´è¼ªå¤±æ•—æ¸…å–®å¯å†è©¦ N è¼ªï¼ˆSMARTEDU_POST_RETRYï¼›é è¨­ 2ï¼›0=é—œé–‰ï¼‰ã€‚
"""
from __future__ import annotations

import os, re, json, asyncio, aiohttp, aiofiles, time, logging
import shutil
from logging import handlers
from pathlib import Path
from urllib.parse import quote
from typing import List, Dict, Any, Tuple, Optional
from collections import namedtuple
from tqdm import tqdm

# ---------------- åŸºæœ¬é…ç½® / å¸¸é‡ ----------------
Settings = namedtuple("Settings", [
    "PHASE","SUBJECTS","FORCE","CATALOG_ONLY","WORKER_URL",
    "HCON","DCON","POST_RETRY"
])

PHASE_TAGS = {
    "å°å­¦": ["å°å­¦"],
    "åˆä¸­": ["åˆä¸­"],
    "é«˜ä¸­": ["é«˜ä¸­", "æ™®é€šé«˜ä¸­"],
    "ç‰¹æ®Šæ•™è‚²": ["ç‰¹æ®Šæ•™è‚²"],
    "å°å­¦54": ["å°å­¦ï¼ˆäº”â€¢å››å­¦åˆ¶ï¼‰", "å°å­¦ï¼ˆäº”Â·å››å­¦åˆ¶ï¼‰"],
    "åˆä¸­54": ["åˆä¸­ï¼ˆäº”â€¢å››å­¦åˆ¶ï¼‰", "åˆä¸­ï¼ˆäº”Â·å››å­¦åˆ¶ï¼‰"],
}

ORDER_SUBJ = ["è¯­æ–‡","æ•°å­¦","è‹±è¯­","ç‰©ç†","åŒ–å­¦","ç”Ÿç‰©","æ€æƒ³æ”¿æ²»","å†å²","åœ°ç†"]
SUBJ_RANK = {v:i for i,v in enumerate(ORDER_SUBJ)}
CLS = {"è¯­æ–‡":"yuwen","æ•°å­¦":"shuxue","è‹±è¯­":"yingyu","ç‰©ç†":"wuli","åŒ–å­¦":"huaxue","ç”Ÿç‰©":"shengwu","æ€æƒ³æ”¿æ²»":"zhengzhi","å†å²":"lishi","åœ°ç†":"dili"}
THEME = {
  "yuwen":   {"chip":"#C2410C","title":"#F59E0B","name":"#F8B76B","grad":"linear-gradient(135deg,#fb923c40,#fed7aa33)","border":"#fb923c","tint":"#2b1a12"},
  "shuxue":  {"chip":"#0D9488","title":"#34D399","name":"#7FE3C8","grad":"linear-gradient(135deg,#14b8a640,#99f6e433)","border":"#2dd4bf","tint":"#10201f"},
  "yingyu":  {"chip":"#2563EB","title":"#60A5FA","name":"#9EC5FF","grad":"linear-gradient(135deg,#3b82f640,#93c5fd33)","border":"#60a5fa","tint":"#121a2b"},
  "wuli":    {"chip":"#7C3AED","title":"#A78BFA","name":"#D2C3FF","grad":"linear-gradient(135deg,#8b5cf640,#c4b5fd33)","border":"#a78bfa","tint":"#191331"},
  "huaxue":  {"chip":"#16A34A","title":"#86EFAC","name":"#BFF5D2","grad":"linear-gradient(135deg,#22c55e40,#bbf7d033)","border":"#86efac","tint":"#0e1e14"},
  "shengwu": {"chip":"#059669","title":"#34D399","name":"#86EBCF","grad":"linear-gradient(135deg,#10b98140,#6ee7b733)","border":"#34d399","tint":"#0c1f1a"},
  "zhengzhi":{"chip":"#D97706","title":"#FBBF24","name":"#FFD683","grad":"linear-gradient(135deg,#f59e0b40,#fde68a33)","border":"#fbbf24","tint":"#261a08"},
  "lishi":   {"chip":"#EA580C","title":"#FB923C","name":"#FFC39C","grad":"linear-gradient(135deg,#f9731640,#fdba7433)","border":"#fb923c","tint":"#29170e"},
  "dili":    {"chip":"#0EA5E9","title":"#67E8F9","name":"#A8F4FE","grad":"linear-gradient(135deg,#06b6d440,#a5f3fc33)","border":"#67e8f9","tint":"#0d1f28"},
}

S_FILE_HOSTS = [
    "https://s-file-1.ykt.cbern.com.cn",
    "https://s-file-2.ykt.cbern.com.cn",
    "https://s-file-3.ykt.cbern.com.cn",
]
R_HOSTS = [
    "https://r1-ndr-oversea.ykt.cbern.com.cn",
    "https://r2-ndr-oversea.ykt.cbern.com.cn",
    "https://r3-ndr-oversea.ykt.cbern.com.cn",
    "https://r1-ndr.ykt.cbern.com.cn",
    "https://r2-ndr.ykt.cbern.com.cn",
    "https://r3-ndr.ykt.cbern.com.cn",
]
ENTRY_PATH = "/zxx/ndrs/resources/tch_material/version/data_version.json"
BASE_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_6_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118 Safari/537.36",
    "Accept": "application/json, text/plain, */*",
    "Accept-Language": "zh-CN,zh;q=0.9",
}

# ---------------- æ—¥èªŒ ----------------
LOGGER = logging.getLogger("smartedu")
def setup_logging(out_dir: Path):
    LOGGER.setLevel(logging.DEBUG)
    LOGGER.handlers.clear()
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(logging.Formatter("[%(asctime)s] %(levelname)s - %(message)s", datefmt="%H:%M:%S"))
    out_dir.mkdir(parents=True, exist_ok=True)
    fh = handlers.RotatingFileHandler(out_dir / "smartedu_download.log", maxBytes=10_000_000, backupCount=2, encoding="utf-8")
    fh.setLevel(logging.DEBUG)
    fh.setFormatter(logging.Formatter("[%(asctime)s] %(levelname)s %(name)s:%(lineno)d - %(message)s", datefmt="%Y-%m-%d %H:%M:%S"))
    LOGGER.addHandler(ch); LOGGER.addHandler(fh)

# ---------------- å·¥å…· ----------------
def esc(s: str) -> str:
    return (s or "").replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")

def have_pdf_head(p: Path) -> bool:
    try:
        if not p.exists() or p.stat().st_size < 100*1024: return False
        with open(p,'rb') as f: return f.read(5) == b'%PDF-'
    except Exception: return False

HEX = r"[0-9a-fA-F]{6,}"
TS  = r"\d{10,14}"
DATE= r"\d{8}"
TAIL_PAT = re.compile(rf"(?:__|_|-)(?:{HEX}|{TS}|{DATE})$", re.IGNORECASE)
PAREN_HASH_TS = re.compile(rf"\((?:{HEX}|{TS}|{DATE})\)$", re.IGNORECASE)

def canon_title(s: str) -> str:
    s = (s or "").strip().replace("ï¼ˆ","(").replace("ï¼‰",")")
    s = PAREN_HASH_TS.sub("", s)
    while True:
        t = TAIL_PAT.sub("", s)
        if t == s: break
        s = t
    return re.sub(r"\s+", " ", s) or "æœªå‘½åæ•™æ"

def canon_filename(name_or_title: str) -> str:
    base = canon_title(name_or_title)
    if not base.lower().endswith(".pdf"): base += ".pdf"
    safe = re.sub(r'[\\/:*?"<>|]', "_", base)
    safe = re.sub(r"\s+", " ", safe)
    return safe

def logic_key(subject: str, name_or_title: str) -> str:
    key = canon_title(name_or_title)
    key = re.sub(r"\s+", "", key)
    key = key.replace("ï¼ˆ","(").replace("ï¼‰",")")
    return f"{subject}::{key}"

def load_settings_from_env() -> Settings:
    pr_raw = os.getenv("SMARTEDU_POST_RETRY", "2").strip()
    try: pr = max(0, min(5, int(pr_raw)))
    except ValueError: pr = 2
    force = os.getenv("SMARTEDU_FORCE", "0") == "1"
    return Settings(
        PHASE=os.getenv("SMARTEDU_PHASE","é«˜ä¸­"),
        SUBJECTS=[s.strip().replace(" ","") for s in os.getenv("SMARTEDU_SUBJ","è¯­æ–‡,æ•°å­¦,è‹±è¯­,æ€æƒ³æ”¿æ²»,å†å²,åœ°ç†,ç‰©ç†,åŒ–å­¦,ç”Ÿç‰©").split(",") if s.strip()],
        HCON=int(os.getenv("SMARTEDU_HCON","12")),
        DCON=int(os.getenv("SMARTEDU_DCON","5")),
        POST_RETRY=pr,
        FORCE=force,
        CATALOG_ONLY=os.getenv("SMARTEDU_CATALOG_ONLY","0")=="1",
        WORKER_URL=os.getenv("SMARTEDU_WORKER_URL","").strip(),
    )

def build_referer(book_id: str) -> str:
    return (f"https://basic.smartedu.cn/tchMaterial/detail"
            f"?contentType=assets_document&contentId={book_id}"
            f"&catalogType=tchMaterial&subCatalog=tchMaterial")

# ---------------- é ç«¯è³‡æºæŠ“å– ----------------
async def get_json(session: aiohttp.ClientSession, url: str) -> Optional[Dict | List]:
    for i in range(3):
        try:
            async with session.get(url, headers=BASE_HEADERS, timeout=30) as resp:
                txt = await resp.text()
                if resp.status == 200 and ("json" in (resp.headers.get("Content-Type","") or "").lower() or txt[:1] in "[{"):
                    return json.loads(txt)
        except (aiohttp.ClientError, asyncio.TimeoutError):
            await asyncio.sleep(1.2 * (i+1))
    return None

async def get_data_urls(session: aiohttp.ClientSession) -> List[str]:
    for base in S_FILE_HOSTS:
        js = await get_json(session, base + ENTRY_PATH)
        if isinstance(js, dict):
            field = js.get("urls") or js.get("url")
            urls: List[str] = []
            if isinstance(field, str):
                urls = [u.strip() for u in field.split(",") if u.strip()]
            elif isinstance(field, list):
                urls = [str(u).strip() for u in field if str(u).strip()]
            if urls: return urls
    return []

def book_tags(book: Dict[str,Any]) -> List[str]:
    return [t.get("tag_name","") for t in (book.get("tag_list") or [])]

def match_phase_subject_keyword(book: Dict[str, Any], st: Settings) -> bool:
    tags = book_tags(book)
    wants = PHASE_TAGS.get(st.PHASE, [])
    if wants and not any(any(w in t for w in wants) for t in tags): return False
    if st.SUBJECTS and not any(any(s in t for s in st.SUBJECTS) for t in tags): return False
    return True

def derive_filename(item: dict, book_id: str) -> Optional[str]:
    stor = item.get("ti_storage") or (item.get("ti_storages") or [None])[0]
    if isinstance(stor, str) and ".pkg/" in stor:
        tail = stor.replace("cs_path:${ref-path}", "").lstrip("/")
        fname = tail.split(".pkg/", 1)[-1]
        base = fname.split("/")[-1] if fname else None
        if base: return base
    fname = item.get("ti_filename")
    if isinstance(fname, str) and fname.lower().endswith(".pdf"):
        return fname.split("/")[-1]
    title = item.get("ti_title") or item.get("title")
    if isinstance(title, str) and title.strip():
        t = title.strip()
        if not t.lower().endswith(".pdf"): t += ".pdf"
        return t
    return None

def candidates_from_detail(book_id: str, items: List[dict]) -> List[str]:
    urls=[]
    for it in items:
        if (it.get("ti_format") or it.get("format") or "").lower() != "pdf": continue
        fname = derive_filename(it, book_id)
        if not fname: continue
        raw = f"esp/assets/{book_id}.pkg/{fname}"
        enc = f"esp/assets/{book_id}.pkg/{quote(fname)}"
        for host in R_HOSTS:
            urls.append(f"{host}/edu_product/{raw}")
            urls.append(f"{host}/edu_product/{enc}")
    for host in R_HOSTS:
        urls.append(f"{host}/edu_product/esp/assets/{book_id}.pkg/pdf.pdf")
    dedup=[]
    seen=set()
    for u in urls:
        if u not in seen: seen.add(u); dedup.append(u)
    return dedup

async def resolve_candidates(session: aiohttp.ClientSession, book_id: str) -> List[str]:
    for base in S_FILE_HOSTS:
        js = await get_json(session, f"{base}/zxx/ndrv2/resources/tch_material/details/{book_id}.json")
        if isinstance(js, dict):
            items = js.get("ti_items") or []
            if items: return candidates_from_detail(book_id, items)
    return [f"{h}/edu_product/esp/assets/{book_id}.pkg/pdf.pdf" for h in R_HOSTS]

async def probe_url(session: aiohttp.ClientSession, url: str, referer: str) -> Tuple[bool, Optional[int]]:
    headers = {**BASE_HEADERS, "Referer": referer}
    try:
        async with session.head(url, headers=headers, timeout=20, allow_redirects=True) as r:
            if r.status == 200:
                cl = r.headers.get("Content-Length")
                ct = (r.headers.get("Content-Type","") or "").lower()
                if ("pdf" in ct or url.lower().endswith(".pdf")) and (cl is None or int(cl) > 50*1024):
                    return True, int(cl) if cl else None
    except Exception:
        pass
    # fallback range GET
    try:
        headers["Range"] = "bytes=0-1"
        async with session.get(url, headers=headers, timeout=20, allow_redirects=True) as r:
            if r.status in (200,206):
                cl = r.headers.get("Content-Length")
                ct = (r.headers.get("Content-Type","") or "").lower()
                return ("pdf" in ct or url.lower().endswith(".pdf")), int(cl) if cl else None
    except Exception:
        return False, None
    return False, None

# ---------------- ä¸‹è¼‰èˆ‡å»é‡ ----------------
def existing_index(out_dir: Path) -> List[Dict[str,Any]]:
    idx = out_dir / "index.json"
    if idx.exists():
        try: return json.loads(idx.read_text("utf-8"))
        except Exception: return []
    return []

def build_existing_map(out_dir: Path) -> Dict[str, Dict[str,Any]]:
    m={}
    for it in existing_index(out_dir):
        subj = it.get("subject") or "ç¶œåˆ"
        key  = logic_key(subj, it.get("title") or Path(it.get("path","")).stem)
        m[key] = it
    # åŒæ™‚å¾ç£ç¢Ÿæƒæè£œå…¨ï¼ˆé¿å…æ‰‹å·¥ç§»å‹•å°è‡´ç´¢å¼•æ¼ï¼‰
    for p in out_dir.rglob("*.pdf"):
        rel = p.relative_to(out_dir).as_posix()
        subj_guess = next((s for s in ORDER_SUBJ if f"/{s}/" in ("/"+rel+"/")), "ç¶œåˆ")
        key = logic_key(subj_guess, p.stem)
        if key not in m:
            m[key] = {"title": canon_title(p.stem), "subject": subj_guess, "phase": "", "path": str(p), "size": p.stat().st_size}
    return m


async def download_pdf(session: aiohttp.ClientSession, url: str, dest: Path, referer: str, force: bool = False) -> bool:
    """Download PDF with exponential backoff retry and optional force overwrite."""
    # Skip if already exists and not forcing
    if not force and have_pdf_head(dest):
        LOGGER.info("å·²å­˜åœ¨æœ‰æ•ˆ PDFï¼Œè·³é: %s", dest.name)
        return True
    
    # Force mode: remove existing file to ensure fresh download
    if force and dest.exists():
        try:
            dest.unlink()
            LOGGER.info("å¼·åˆ¶æ¨¡å¼ï¼šåˆªé™¤èˆŠç‰ˆæœ¬ %s", dest.name)
        except Exception as e:
            LOGGER.warning("åˆªé™¤èˆŠæ–‡ä»¶å¤±æ•—: %s (%s)", dest.name, e)
    
    tmp = dest.with_suffix(".part")
    start = tmp.stat().st_size if tmp.exists() else 0
    headers = {**BASE_HEADERS, "Referer": referer}
    if start > 0 and not force:
        headers["Range"] = f"bytes={start}-"
    elif force and tmp.exists():
        # Force mode: start fresh
        tmp.unlink()
        start = 0
    
    max_retries = 4
    for attempt in range(max_retries):
        # Exponential backoff: 1s, 2s, 4s, 8s
        backoff = 2 ** attempt
        try:
            async with session.get(url, headers=headers, timeout=180) as r:
                if r.status not in (200, 206):
                    LOGGER.debug("ä¸‹è¼‰ HTTP %s: %s", r.status, url)
                    await asyncio.sleep(backoff)
                    continue
                
                dest.parent.mkdir(parents=True, exist_ok=True)
                mode = "ab" if (start > 0 and r.status == 206) else "wb"
                total_size = int(r.headers.get("Content-Length", 0)) + start
                downloaded = start
                
                async with aiofiles.open(tmp, mode) as f:
                    async for chunk in r.content.iter_chunked(1 << 14):
                        await f.write(chunk)
                        downloaded += len(chunk)
                
                # Validate minimum file size (at least 100KB for a real PDF)
                if tmp.stat().st_size < 100 * 1024:
                    LOGGER.warning("ä¸‹è¼‰æ–‡ä»¶éå°ï¼Œå¯èƒ½æå£: %s (%d bytes)", dest.name, tmp.stat().st_size)
                    await asyncio.sleep(backoff)
                    continue
                
                tmp.replace(dest)
                if have_pdf_head(dest):
                    LOGGER.info("âœ… ä¸‹è¼‰å®Œæˆ: %s (%.1f MB)", dest.name, dest.stat().st_size / 1024 / 1024)
                    return True
                else:
                    LOGGER.warning("ä¸‹è¼‰å®Œæˆä½†éæœ‰æ•ˆ PDF: %s", dest.name)
                    return False
                    
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            LOGGER.debug("ä¸‹è¼‰ç•°å¸¸ (%d/%d) %s - ç­‰å¾… %ds", attempt + 1, max_retries, e, backoff)
            await asyncio.sleep(backoff)
        except Exception as e:
            LOGGER.warning("ä¸‹è¼‰æœªé æœŸéŒ¯èª¤: %s (%s)", url, e)
            await asyncio.sleep(backoff)
    
    LOGGER.warning("ä¸‹è¼‰å¤±æ•—ï¼ˆå·²é‡è©¦ %d æ¬¡ï¼‰: %s", max_retries, url)
    return False

# ---------------- HTML ç”Ÿæˆå·²ç§»é™¤ï¼ˆåƒ…ä¿ç•™ä¸‹è¼‰åŠŸèƒ½ï¼‰ ----------------
# å¦‚éœ€ç¶²ç«™åŠŸèƒ½ï¼Œè«‹ä½¿ç”¨ç¨ç«‹çš„ web ç”Ÿæˆå·¥å…·

# ---------------- ä¸»æµç¨‹ ----------------
async def resolve_all_books(session: aiohttp.ClientSession, st: Settings) -> List[Dict[str,Any]]:
    LOGGER.info("ğŸ” è®€å–é ç¨‹ç´¢å¼•...")
    urls = await get_data_urls(session)
    if not urls:
        LOGGER.error("ç„¡æ³•ç²å– data_version.json çš„ urlsã€‚"); return []
    books: List[Dict[str,Any]] = []
    for url in urls:
        js = await get_json(session, url)
        if isinstance(js, list): books.extend(js)
    books = [b for b in books if match_phase_subject_keyword(b, st)]
    LOGGER.info("ç›®æ¨™æ¢ç›®: %d", len(books))
    return books

async def resolve_one_book(session: aiohttp.ClientSession, book: Dict[str,Any], st: Settings, sem: asyncio.Semaphore) -> Optional[Tuple[str,str,str,str,Optional[int]]]:
    """Resolve a single book's download URL with semaphore limiting."""
    async with sem:
        bid = book.get("id") or book.get("content_id")
        if not bid:
            return None
        title = canon_title(book.get("title") or (book.get("global_title") or {}).get("zh-CN") or bid)
        subj = next((s for s in st.SUBJECTS if any(s in t for t in book_tags(book))), "ç¶œåˆ")
        ref = build_referer(bid)
        urls = await resolve_candidates(session, bid)
        for u in urls:
            ok, rlen = await probe_url(session, u, ref)
            if ok:
                return (bid, title, subj, u, rlen)
        return None

async def main():
    st = load_settings_from_env()
    out_dir = Path("./smartedu_textbooks")
    setup_logging(out_dir)
    LOGGER.info("ğŸ“ ä¸‹è¼‰ç›®éŒ„: %s", out_dir)
    LOGGER.info("éšæ®µ=%s | å­¸ç§‘=%s | å¼·åˆ¶è¦†è“‹=%s | è‡ªå‹•é‡è©¦è¼ª=%d | åƒ…ç›®éŒ„æ¨¡å¼=%s",
                st.PHASE, ",".join(st.SUBJECTS), st.FORCE, st.POST_RETRY, st.CATALOG_ONLY)

    timeout = aiohttp.ClientTimeout(total=None, sock_connect=20, sock_read=180)
    async with aiohttp.ClientSession(timeout=timeout) as session:
        books = await resolve_all_books(session, st)
        
        if not books:
            LOGGER.warning("æ²’æœ‰åŒ¹é…çš„æ¢ç›®ã€‚")

        # æ§‹å»ºç¾æœ‰æ–‡ä»¶æ˜ å°„
        exist_map = build_existing_map(out_dir)

        # ========= çœŸæ­£ä¸¦ç™¼è§£æç›´éˆ =========
        sem = asyncio.Semaphore(st.HCON)
        queue: List[Tuple[str,str,str,str,Optional[int]]] = []
        
        if books:
            LOGGER.info("ğŸ”— ä¸¦ç™¼è§£æ %d æœ¬æ•™æçš„ä¸‹è¼‰éˆæ¥ï¼ˆä¸¦ç™¼æ•¸: %dï¼‰...", len(books), st.HCON)
            
            tasks = [resolve_one_book(session, b, st, sem) for b in books]
            pbar = tqdm(total=len(tasks), desc="è§£æç›´éˆ", unit="æœ¬")
            
            for coro in asyncio.as_completed(tasks):
                result = await coro
                pbar.update(1)
                if result:
                    queue.append(result)
            
            pbar.close()
            LOGGER.info("âœ… æˆåŠŸè§£æ %d / %d æœ¬", len(queue), len(books))

        failures: List[Dict[str,Any]] = []
        if st.CATALOG_ONLY:
            LOGGER.info("ğŸ“š åƒ…ç›®éŒ„æ¨¡å¼ï¼šè·³éä¸‹è¼‰ï¼Œç”Ÿæˆç›´æ¥éˆæ¥...")
            for bid, title, subj, url, rlen in queue:
                key = logic_key(subj, title)
                # è¨˜éŒ„ç‚ºé ç¨‹é …ç›®
                exist_map[key] = {
                    "title": title, "subject": subj, "phase": st.PHASE,
                    "url": url, "size": rlen or 0, "is_remote": True,
                    "referer": build_referer(bid),
                    "path": f"REMOTE/{bid}/{canon_filename(title)}" # è™›æ“¬è·¯å¾‘
                }
        else:
            # ä¸‹è¼‰ï¼ˆæ”¯æŒæ–·é»èˆ‡è·³éï¼‰ï¼ŒæŒ‰ DCON æ§åˆ¶ä¸¦ç™¼
            async def worker(items):
                for bid, title, subj, url, rlen in items:
                    # ç›®éŒ„ï¼šout/å­¸æ®µ/å­¸ç§‘/
                    dest_dir = out_dir / st.PHASE / subj
                    dest_dir.mkdir(parents=True, exist_ok=True)
                    dest = dest_dir / canon_filename(title)
                    key  = logic_key(subj, title)
    
                    # å¼·åˆ¶æ¨¡å¼è·³éæ‰€æœ‰å­˜åœ¨æ€§æª¢æŸ¥
                    if not st.FORCE:
                        # è‹¥å·²æœ‰ç›¸åŒ key çš„æ–‡ä»¶ï¼ˆä»»ä½•å­¸æ®µï¼‰ï¼Œä¸”æª”æ¡ˆæœ‰æ•ˆã€å¤§å° >= é ç«¯ï¼ˆè‹¥å·²çŸ¥ï¼‰ï¼Œè·³é
                        exist = exist_map.get(key)
                        if exist:
                            p = Path(exist.get("path",""))
                            p = (out_dir/p) if not p.is_absolute() else p
                            if p.exists() and have_pdf_head(p):
                                if rlen is None or p.stat().st_size >= rlen:
                                    LOGGER.info("è·³éï¼ˆå·²å­˜åœ¨æ›´å¤§/ç›¸ç­‰ï¼‰: %s", title)
                                    continue
    
                        # è‹¥ç›®æ¨™è·¯å¾‘å·²æœ‰æœ‰æ•ˆ PDFï¼Œäº¦è·³é
                        if have_pdf_head(dest):
                            LOGGER.info("è·³éï¼ˆæœ¬åœ°å·²å®Œæ•´ï¼‰: %s", dest.name)
                            continue
    
                    ok = await download_pdf(session, url, dest, build_referer(bid), force=st.FORCE)
                    if ok:
                        exist_map[key] = {"title": title, "subject": subj, "phase": st.PHASE, "path": str(dest.relative_to(out_dir)), "size": dest.stat().st_size}
                    else:
                        failures.append({"id": bid, "title": title, "subject": subj, "phase": st.PHASE, "url": url})
    
                # æ‹†åˆ†çµ¦ DCON å€‹ worker
                if queue:
                    chunks = [queue[i::max(1,st.DCON)] for i in range(max(1,st.DCON))]
                    tasks = [asyncio.create_task(worker(ch)) for ch in chunks]
                    await asyncio.gather(*tasks)
    
                # è‡ªå‹•é‡è©¦è¼ªï¼šåªé‡å°å¤±æ•—æ¸…å–®ï¼Œå†è·‘ st.POST_RETRY è¼ª
                for round_i in range(st.POST_RETRY):
                    if not failures: break
                    LOGGER.info("â™»ï¸ è‡ªå‹•é‡è©¦è¼ª %d / %dï¼Œå‰©é¤˜ %d æœ¬", round_i+1, st.POST_RETRY, len(failures))
                    retrying = failures; failures=[]
                    # é‡æ–°è§£æ+ä¸‹è¼‰
                    q2=[]
                    for f in retrying:
                        bid=f["id"]; title=f["title"]; subj=f["subject"]; ref=build_referer(bid)
                        urls = await resolve_candidates(session, bid)
                        chosen=None; rlen=None
                        for u in urls:
                            ok, rlen = await probe_url(session, u, ref)
                            if ok: chosen=u; break
                        if chosen: q2.append((bid,title,subj,chosen,rlen))
                    if q2:
                        chunks = [q2[i::max(1,st.DCON)] for i in range(max(1,st.DCON))]
                        tasks = [asyncio.create_task(worker(ch)) for ch in chunks]
                        await asyncio.gather(*tasks)

        # â€”â€” ç”Ÿæˆ index.json èˆ‡é é¢ â€”â€” 
        items = []
        
        # éæ­· exist_map (åŒ…å«æœ¬åœ°èˆ‡é ç¨‹)
        # Scan out_dir to be sure about local files? 
        # But exist_map is updated during download. 
        # Let's trust exist_map + pure scan to be safe? 
        # The safest way is to rebuild exist_map from disk for local files if not skipping checks.
        # But we just downloaded. 
        # Let's iterate exist_map.
        
        for k, v in exist_map.items():
            if v.get("is_remote"):
                items.append(v)
            else:
                p = Path(v.get("path",""))
                abs_p = (out_dir/p) if not p.is_absolute() else p
                if abs_p.exists() and have_pdf_head(abs_p):
                    v["size"] = abs_p.stat().st_size
                    v["title"]= canon_title(v.get("title") or abs_p.stem)
                    try:
                       rel = abs_p.relative_to(out_dir).as_posix()
                    except:
                       rel = str(abs_p)
                    v["path"] = rel
                    items.append(v)
        
        # ä¿å­˜ç´¢å¼•
        items.sort(key=lambda x: (x.get("subject",""), x.get("title","")))
        (out_dir / "index.json").write_text(json.dumps(items, ensure_ascii=False, indent=2), "utf-8")
        LOGGER.info("ğŸ“ ç´¢å¼•å·²ä¿å­˜: %s/index.json (å…± %d æ¢)", out_dir, len(items))

        # ç¶²ç«™ç”Ÿæˆå·²ç§»é™¤ - åƒ…ä¿ç•™ä¸‹è¼‰å’Œç´¢å¼•åŠŸèƒ½
        
        if failures:
            LOGGER.error("âŒ ä»¥ä¸‹ %d æœ¬ä¸‹è¼‰å¤±æ•— (å·²é‡è©¦ %d è¼ª):", len(failures), st.POST_RETRY)
            for f in failures:
                LOGGER.error("   [%s] %s | %s", f["subject"], f["title"], f["id"])
            (out_dir / "failed.json").write_text(json.dumps(failures, ensure_ascii=False, indent=2), "utf-8")
            LOGGER.warning("ä»å¤±æ•— %d æœ¬ï¼›è©³è¦‹ failed.jsonï¼Œå¯ç”¨ -R åƒ…é‡è©¦å¤±æ•—ã€‚", len(failures))
        else:
            try: (out_dir/"failed.json").unlink()
            except FileNotFoundError: pass
            if not st.CATALOG_ONLY and queue:
                LOGGER.info("ğŸ‰ æ‰€æœ‰ä»»å‹™å®Œæˆï¼")
            else:
                LOGGER.info("âœ… æœ¬è¼ªå…¨éƒ¨æˆåŠŸæˆ–å·²å­˜åœ¨ï¼ˆå»é‡è·³éï¼‰ã€‚")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass