# =============================================================================
# smartedu_fetch_all.sh  â€”â€”  Shell + Python polyglot (v1.0)
# -----------------------------------------------------------------------------
# æ–°å¢ï¼š
#  â€¢ æ•´è¼ªçµæŸå¾Œè‡ªå‹•é‡è©¦ï¼ˆé è¨­ 2 è¼ªï¼Œå¯ç”¨ -T N èª¿æ•´ï¼›0 è¡¨ç¤ºé—œé–‰ï¼‰ã€‚
#  â€¢ æ¯è¼ªé‡è©¦åƒ…é‡å°ä¸Šè¼ªå¤±æ•—æ¸…å–®ï¼ŒæˆåŠŸå³å¯«å› index.jsonï¼Œä»å¤±æ•—ä¿ç•™åˆ° failed.jsonã€‚
#  â€¢ æœ€çµ‚è‹¥ä»æœ‰å¤±æ•—ï¼Œæ¸…æ™°æç¤ºç”¨æˆ¶å¯å†æ¬¡é‹è¡Œæˆ–ç”¨ -R åƒ…é‡è©¦å¤±æ•—ã€‚
# å…¶ä»–ï¼š
#  â€¢ ä¿æŒ v4.2 çš„ç©©å®šæ€§ï¼šå¤šä¸»æ©Ÿç´¢å¼•/è©³æƒ…ã€Referer å®Œæ•´ã€HEAD/Range æ¢æ¸¬ã€æ–·é»çºŒå‚³ã€å»é‡ã€è©³ç›¡æ—¥èªŒã€‚
# -----------------------------------------------------------------------------
# ç”¨æ³•ï¼š
#   bash smartedu_fetch_all.sh -p é«˜ä¸­
#   bash smartedu_fetch_all.sh -p é«˜ä¸­ -s è¯­æ–‡,æ•°å­¦ -m "å¿…ä¿® ç¬¬ä¸€å†Œ" -T 3
#   bash smartedu_fetch_all.sh -R -o ./output_dir
# =============================================================================

# ---- å¦‚æœä»¥ python æ–¹å¼èª¿ç”¨ï¼Œç›´æ¥è·³é Shell éƒ¨åˆ† ----
if [ -n "${PYTHON_EXEC:-}" ]; then
  :
else
  set -euo pipefail

  # èª¿è©¦æ¨¡å¼ï¼šDEBUG=1 æ™‚æ‰“å°åŸ·è¡Œç´°ç¯€
  if [ "${DEBUG:-0}" = "1" ]; then set -x; fi

  # è¨˜éŒ„ç•¶å‰å·¥ä½œç›®éŒ„ï¼Œé¿å… /dev/fd è·¯å¾‘é€ æˆç›¸å°è·¯å¾‘æ··äº‚
  PWD_ABS="$(pwd)"

  # å°æ–¼ apt ç³»çµ±ï¼Œé è¨­ç‚ºéäº’å‹•æ¨¡å¼ï¼Œé¿å…å®‰è£ä¸­é€”åœä¸‹
  if command -v apt-get >/dev/null 2>&1 || command -v apt >/dev/null 2>&1; then
    export DEBIAN_FRONTEND=${DEBIAN_FRONTEND:-noninteractive}
  fi

  # é è¨­åƒæ•¸
  PHASE="é«˜ä¸­"
  SUBJECTS="è¯­æ–‡,æ•°å­¦,è‹±è¯­,æ€æƒ³æ”¿æ²»,å†å²,åœ°ç†,ç‰©ç†,åŒ–å­¦,ç”Ÿç‰©"
  MATCH=""
  IDS=""
  OUT_DIR=""
  ONLY_FAILED="0"
  HCON=12
  DCON=5
  LIMIT=""
  POST_RETRY=2
  AUTO_RUN="0"

  usage() {
    cat >&2 <<'USAGE'
ç”¨æ³•:
  bash smartedu_fetch_all.sh [é¸é …]

é¸é …:
  -p PHASE         æ•™è‚²éšæ®µï¼šå°å­¦|åˆä¸­|é«˜ä¸­|ç‰¹æ®Šæ•™è‚²|å°å­¦54|åˆä¸­54    (é è¨­: é«˜ä¸­)
  -s SUBJECTS      å­¸ç§‘é€—è™Ÿåˆ†éš” (é è¨­: è¯­æ–‡,æ•°å­¦,è‹±è¯­,æ€æƒ³æ”¿æ²»,å†å²,åœ°ç†,ç‰©ç†,åŒ–å­¦,ç”Ÿç‰©)
  -m KEYWORD       æ›¸åé—œéµè©ï¼ˆå­ä¸²åŒ¹é…ï¼Œå¯å¤šè©ä»¥ç©ºæ ¼åˆ†éš”ï¼‰
  -i IDS           æŒ‡å®š contentId åˆ—è¡¨ï¼Œé€—è™Ÿåˆ†éš”ï¼ˆè·³éç´¢å¼•éæ¿¾ï¼Œç›´é”ï¼‰
  -o OUT_DIR       è‡ªå®šç¾©è¼¸å‡ºç›®éŒ„ï¼ˆé è¨­: ./smartedu_textbooksï¼‰
  -R               åƒ…é‡è©¦ä¸Šæ¬¡å¤±æ•—ï¼ˆè®€ OUT_DIR/failed.jsonï¼‰
  -c N             è§£æç›´éˆä¸¦ç™¼ (HEAD æª¢æŸ¥)ï¼Œé è¨­ 12
  -d N             ä¸‹è¼‰ä¸¦ç™¼ï¼Œé è¨­ 5
  -n N             åªè™•ç†å‰ N æœ¬ï¼ˆèª¿è©¦ç”¨ï¼‰
  -T N             æ•´è¼ªçµæŸå¾Œè‡ªå‹•é‡è©¦ N è¼ªï¼ˆé è¨­ 2ï¼›0=é—œé–‰ï¼‰
  -y               éäº’å‹•ç›´è·‘ï¼ˆè·³éäº¤äº’é¸æ“‡ï¼Œä½¿ç”¨ç•¶å‰åƒæ•¸ï¼‰
  -h               é¡¯ç¤ºæ­¤å¹«åŠ©

ç¤ºä¾‹:
  bash smartedu_fetch_all.sh -p é«˜ä¸­
  bash smartedu_fetch_all.sh -p é«˜ä¸­ -s è¯­æ–‡,æ•°å­¦ -m "å¿…ä¿® ç¬¬ä¸€å†Œ" -T 3
  bash smartedu_fetch_all.sh -p å°å­¦54 -o ~/Downloads/textbooks
USAGE
  }

  while getopts ":p:s:m:i:o:w:Rc:d:n:T:hy" opt; do
    case "$opt" in
      p) PHASE="$OPTARG" ;;
      s) SUBJECTS="$OPTARG" ;;
      m) MATCH="$OPTARG" ;;
      i) IDS="$OPTARG" ;;
      o) OUT_DIR="$OPTARG" ;;
      w) WEB_DIR="$OPTARG" ;;
      R) ONLY_FAILED="1" ;;
      c) HCON="$OPTARG" ;;
      d) DCON="$OPTARG" ;;
      n) LIMIT="$OPTARG" ;;
      T) POST_RETRY="$OPTARG" ;;
      y) AUTO_RUN="1" ;;
      h) usage; exit 0 ;;
      :) echo "éŒ¯èª¤ï¼šé¸é … -$OPTARG éœ€è¦ä¸€å€‹åƒæ•¸ã€‚" >&2; usage; exit 2 ;;
      \?) echo "éŒ¯èª¤ï¼šæœªçŸ¥é¸é … -$OPTARG" >&2; usage; exit 2 ;;
    esac
  done

  # ---- äº¤äº’å¼é…ç½®ï¼ˆé»˜èªé–‹å•Ÿï¼›ç”¨ -y è·³éï¼‰ ----
  if [ "$AUTO_RUN" != "1" ] && [ -t 0 ]; then
    printf "\n================ ä¸‹è¼‰é…ç½®åš®å° ================\n"
    printf "åªéœ€é¸æ“‡ æ•™è‚²éšæ®µ å’Œ å­¸ç§‘ï¼›å…¶é¤˜ä¿æŒé»˜èªä¸¦è‡ªå‹•é–‹å§‹ã€‚\n"

    # æ•™è‚²éšæ®µï¼ˆæ•¸å­—é¸æ“‡ï¼‰
    printf "\n[1] æ•™è‚²éšæ®µï¼š\n"
    printf "   1) å°å­¦    2) åˆä¸­    3) é«˜ä¸­    4) ç‰¹æ®Šæ•™è‚²    5) å°å­¦54    6) åˆä¸­54\n"
    read -r -p "è¼¸å…¥æ•¸å­— 1-6ï¼ˆé»˜èª: $PHASEï¼‰: " ans
    case "$ans" in
      1) PHASE="å°å­¦";;
      2) PHASE="åˆä¸­";;
      3) PHASE="é«˜ä¸­";;
      4) PHASE="ç‰¹æ®Šæ•™è‚²";;
      5) PHASE="å°å­¦54";;
      6) PHASE="åˆä¸­54";;
      "") : ;;
      *) printf "[i] éæ³•é¸æ“‡ï¼Œä¿æŒ: %s\n" "$PHASE";;
    esac

    # å­¸ç§‘ï¼ˆåƒ…æ­¤ä¸€æ­¥ï¼›ç•™ç©ºæ²¿ç”¨ç•¶å‰é è¨­ï¼‰
    printf "\n[2] å­¸ç§‘ï¼ˆé€—è™Ÿåˆ†éš”ï¼Œç•™ç©º=å…¨éƒ¨é è¨­ï¼‰\n"
    printf "    ç•¶å‰: %s\n" "$SUBJECTS"
    read -r -p "è¼¸å…¥å­¸ç§‘: " ans
    [ -n "$ans" ] && SUBJECTS="$ans"

    # ç›´æ¥é–‹å§‹â€”â€”ä¸å†è©¢å•ï¼šåƒ…é‡è©¦/é™åˆ¶/é‡è©¦è¼ªæ•¸/è¼¸å‡ºç›®éŒ„/ç¢ºèª
  fi

  # äº¤äº’è¼¸å…¥å¾Œå†åšä¸€æ¬¡æ•¸å€¼æ ¡é©—
  int_re='^[0-9]+$'
  if ! [[ "$HCON" =~ $int_re ]]; then echo "[!] -c å¿…é ˆç‚ºæ•´æ•¸" >&2; exit 2; fi
  if ! [[ "$DCON" =~ $int_re ]]; then echo "[!] -d å¿…é ˆç‚ºæ•´æ•¸" >&2; exit 2; fi
  if [[ -n "$LIMIT" ]] && ! [[ "$LIMIT" =~ $int_re ]]; then echo "[!] -n å¿…é ˆç‚ºæ•´æ•¸" >&2; exit 2; fi
  if ! [[ "$POST_RETRY" =~ $int_re ]]; then echo "[!] -T å¿…é ˆç‚ºæ•´æ•¸" >&2; exit 2; fi

  # --- æ¬Šé™èˆ‡åŒ…ç®¡ç†å™¨åµæ¸¬ ---
  if [ "${EUID:-$(id -u)}" -ne 0 ]; then SUDO="sudo"; else SUDO=""; fi
  have() { command -v "$1" >/dev/null 2>&1; }
  pm=""
  if have apt-get; then pm=apt; elif have apt; then pm=apt; elif have dnf; then pm=dnf; elif have yum; then pm=yum; elif have pacman; then pm=pacman; elif have zypper; then pm=zypper; elif have apk; then pm=apk; elif have brew; then pm=brew; fi

  # --- å®‰è£ Python èˆ‡ pip/venvï¼Œæ¶µè“‹ä¸»æµç™¼è¡Œç‰ˆ ---
  install_python() {
    echo "[*] æº–å‚™ Python ç’°å¢ƒ... (pkgmgr=$pm)"
    case "$pm" in
      apt)
        $SUDO apt-get update -y -qq || true
        if [ -n "$SUDO" ]; then
          $SUDO env DEBIAN_FRONTEND=${DEBIAN_FRONTEND:-noninteractive} \
            apt-get install -y -qq \
              -o Dpkg::Options::=--force-confdef \
              -o Dpkg::Options::=--force-confnew \
              python3 python3-venv python3-pip ca-certificates
        else
          env DEBIAN_FRONTEND=${DEBIAN_FRONTEND:-noninteractive} \
            apt-get install -y -qq \
              -o Dpkg::Options::=--force-confdef \
              -o Dpkg::Options::=--force-confnew \
              python3 python3-venv python3-pip ca-certificates
        fi
        ;;
      dnf)
        $SUDO dnf install -y python3 python3-pip
        ;;
      yum)
        $SUDO yum install -y python3 python3-pip
        ;;
      pacman)
        $SUDO pacman -Sy --noconfirm python python-pip
        ;;
      zypper)
        $SUDO zypper -n install python3 python3-pip
        ;;
      apk)
        $SUDO apk add --no-cache python3 py3-pip ca-certificates
        ;;
      brew)
        brew update >/dev/null || true
        brew install python || true
        ;;
      *)
        echo "[!] æœªè­˜åˆ¥çš„åŒ…ç®¡ç†å™¨ï¼Œè«‹æ‰‹å‹•å®‰è£ python3/pipã€‚" >&2
        ;;
    esac

    # è‹¥ç¼º ensurepipï¼Œå˜—è©¦ä¿®å¾©
    if ! python3 - <<'PY' 2>/dev/null
import ensurepip; print('ok')
PY
    then
      echo "[*] å˜—è©¦å•Ÿç”¨ ensurepip..."
      python3 -m ensurepip --upgrade >/dev/null 2>&1 || true
    fi

    # è‹¥ä»ç„¡ pipï¼Œä½¿ç”¨ get-pip å¼•å°
    if ! python3 -m pip --version >/dev/null 2>&1; then
      echo "[*] ä½¿ç”¨ get-pip å¼•å°å®‰è£ pip..."
      TMPPIP="$(mktemp -t getpip_XXXX).py"
      if have curl; then curl -fsSL https://bootstrap.pypa.io/get-pip.py -o "$TMPPIP"; elif have wget; then wget -qO "$TMPPIP" https://bootstrap.pypa.io/get-pip.py; else echo "[!] éœ€è¦ curl æˆ– wget ä¸‹è¼‰ get-pip.py" >&2; exit 1; fi
      python3 "$TMPPIP" >/dev/null
      rm -f "$TMPPIP"
    fi
  }

  if ! have python3; then
    if [ -z "$pm" ]; then echo "[!] æœªæª¢æ¸¬åˆ°åŒ…ç®¡ç†å™¨ä¸”ç³»çµ±ç„¡ python3ï¼Œè«‹å…ˆæ‰‹å‹•å®‰è£ã€‚" >&2; exit 1; fi
    install_python
  else
    # æŸäº› Debian/Ubuntu ç²¾ç°¡é¡åƒé›–æœ‰ python3 ä½†ç¼º venv æ¨¡å¡Š
    if [ "$pm" = apt ] && ! python3 -c 'import venv' 2>/dev/null; then
      echo "[*] å®‰è£ python3-venv ..."; $SUDO apt-get update -y -qq; \
      if [ -n "$SUDO" ]; then
        $SUDO env DEBIAN_FRONTEND=${DEBIAN_FRONTEND:-noninteractive} apt-get install -y -qq python3-venv
      else
        env DEBIAN_FRONTEND=${DEBIAN_FRONTEND:-noninteractive} apt-get install -y -qq python3-venv
      fi
    fi
    # è‹¥ç„¡ pip äº¦è£œé½Š
    if ! python3 -m pip --version >/dev/null 2>&1; then
      install_python
    fi
  fi

  # --- å»ºç«‹è™›æ“¬ç’°å¢ƒï¼ˆå¤±æ•—å‰‡ä¿®å¾©å¾Œé‡è©¦ï¼Œä»å¤±æ•— fallback ç³»çµ± Pythonï¼‰ ---
  # å…è¨±å¤–éƒ¨å¼·åˆ¶ä½¿ç”¨ç³»çµ± Pythonï¼šUSE_SYSTEM_PY=1 bash jks.sh ...
  if [ "${USE_SYSTEM_PY:-0}" = "1" ]; then
    echo "[i] å·²æŒ‡å®š USE_SYSTEM_PY=1ï¼Œè·³é venv æ§‹å»ºï¼Œç›´æ¥ä½¿ç”¨ç³»çµ± Pythonã€‚"
  fi

  VENV_DIR="${VENV_DIR:-$PWD_ABS/.venv}"
  if [ "${USE_SYSTEM_PY:-0}" != "1" ]; then
    if [ ! -d "$VENV_DIR" ]; then
      echo "[*] å‰µå»ºè™›æ“¬ç’°å¢ƒ $VENV_DIR"
      if ! python3 -m venv "$VENV_DIR" 2>/tmp/venv.err; then
        echo "[!] venv å»ºç«‹å¤±æ•—ï¼Œå˜—è©¦ä¿®å¾©..."
        if [ "$pm" = apt ]; then
          if [ -n "$SUDO" ]; then
            $SUDO env DEBIAN_FRONTEND=${DEBIAN_FRONTEND:-noninteractive} apt-get install -y -qq python3-venv || true
          else
            env DEBIAN_FRONTEND=${DEBIAN_FRONTEND:-noninteractive} apt-get install -y -qq python3-venv || true
          fi
        fi
        python3 -m ensurepip --upgrade >/dev/null 2>&1 || true
        if ! python3 -m venv "$VENV_DIR" 2>>/tmp/venv.err; then
          echo "[!] ä»ç„¡æ³•å»ºç«‹ venvï¼Œå°‡æ”¹ç”¨ç³»çµ± Python ç¹¼çºŒï¼ˆå»ºè­°ç¨å¾Œä¿®å¾© venvï¼‰ã€‚" >&2
          USE_SYSTEM_PY=1
        fi
      fi
    fi
  fi

  if [ "${USE_SYSTEM_PY:-0}" != "1" ]; then
    if [ -f "$VENV_DIR/bin/activate" ] && [ -x "$VENV_DIR/bin/python3" ]; then
      # shellcheck disable=SC1091
      . "$VENV_DIR/bin/activate"
      echo "[i] å·²å•Ÿç”¨è™›æ“¬ç’°å¢ƒï¼š$VENV_DIR"
    else
      echo "[!] venv æ§‹å»ºä¸å®Œæ•´ï¼Œæ‰¾ä¸åˆ° $VENV_DIR/bin/activate æˆ– python3ï¼›å°‡æ”¹ç”¨ç³»çµ± Python ç¹¼çºŒã€‚" >&2
      USE_SYSTEM_PY=1
      if [ -f /tmp/venv.err ]; then
        echo "[i] venv å»ºç«‹éŒ¯èª¤æ‘˜è¦ï¼š" >&2
        tail -n 50 /tmp/venv.err >&2 || true
      fi
      echo "[i] ç•¶å‰å·¥ä½œç›®éŒ„ï¼š$PWD_ABSï¼›VENV_DIR=$VENV_DIR"
      echo "[i] ç›®éŒ„åˆ—èˆ‰ï¼š"; ls -la "$PWD_ABS" || true
    fi
  fi

  # å®‰è£ä¾è³´
  echo "[i] ä½¿ç”¨çš„ Python: $(command -v python3)"
  python3 --version || true
  python3 -m pip install -U pip wheel setuptools >/dev/null
  python3 -m pip install -U aiohttp aiofiles tqdm >/dev/null

  export SMARTEDU_PHASE="$PHASE"
  export SMARTEDU_SUBJ="$SUBJECTS"
  export SMARTEDU_MATCH="$MATCH"
  export SMARTEDU_IDS="$IDS"
  # --- ç¢ºå®šè¼¸å‡ºç›®éŒ„ï¼ˆé è¨­å„ªå…ˆ /srv/smartedu_textbooksï¼›å¦å‰‡ç”¨ç›¸å°ç›®éŒ„ï¼‰ ---
  if [ -z "$OUT_DIR" ]; then
    if [ -d /srv/smartedu_textbooks ] || [ -w /srv ]; then
      OUT_DIR="/srv/smartedu_textbooks"
      mkdir -p "$OUT_DIR"
    else
      OUT_DIR="./smartedu_textbooks"
    fi
  fi
  export SMARTEDU_OUT_DIR="$OUT_DIR"
  echo "[i] ä¸‹è¼‰è¼¸å‡ºç›®éŒ„: $SMARTEDU_OUT_DIR"

  # --- ç¢ºå®šç¶²é æ ¹ç›®éŒ„ï¼ˆWEB_DIRï¼‰ï¼šæœªæŒ‡å®šå‰‡é»˜èª /srv/smartedu_textbooksï¼Œå¦å‰‡æ²¿ç”¨ OUT_DIR ---
  if [ -z "${WEB_DIR:-}" ]; then
    if [ -d /srv/smartedu_textbooks ] || [ -w /srv ]; then
      WEB_DIR="/srv/smartedu_textbooks"
      mkdir -p "$WEB_DIR"
    else
      WEB_DIR="$OUT_DIR"
    fi
  fi
  export SMARTEDU_WEB_DIR="$WEB_DIR"
  echo "[i] ç¶²é æ ¹ç›®éŒ„: $SMARTEDU_WEB_DIR"
  export SMARTEDU_ONLY_FAILED="$ONLY_FAILED"
  export SMARTEDU_HCON="$HCON"
  export SMARTEDU_DCON="$DCON"
  export SMARTEDU_LIMIT="$LIMIT"
  export SMARTEDU_POST_RETRY="$POST_RETRY"
  export SMARTEDU_WEB_DIR="$WEB_DIR"
  export PYTHON_EXEC=1

  # --- é…ç½® Nginx PDF è¨ªå•å°ˆç”¨æ—¥èªŒï¼ˆè‹¥ç³»çµ±æœ‰ nginxï¼‰ ---
  setup_nginx_pdf_logging() {
    if ! command -v nginx >/dev/null 2>&1; then return; fi
    local cfg="/etc/nginx/conf.d/textbook_pdf_logging.conf"
    if [ -f "$cfg" ]; then
      echo "[i] Nginx PDF logging å·²å­˜åœ¨: $cfg"; return;
    fi
    echo "[*] é…ç½® Nginx PDF å°ˆç”¨è¨ªå•æ—¥èªŒ..."
    $SUDO tee "$cfg" >/dev/null <<'NG'
# åœ¨ http å€å¡Šç”Ÿæ•ˆï¼šæŒ‰è«‹æ±‚ URI æ˜¯å¦ç‚º .pdf æ±ºå®šæ˜¯å¦è¨˜éŒ„
map $request_uri $is_textbook_pdf {
  default 0;
  ~*\.pdf$ 1;
}
log_format textbook '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" "$http_cf_connecting_ip" '
                    'host=$host uri=$request_uri bytes=$bytes_sent '
                    'sent_type=$sent_http_content_type';
access_log /var/log/nginx/textbook_access.log textbook if=$is_textbook_pdf;
NG
    $SUDO nginx -t && $SUDO systemctl reload nginx || echo "[!] Nginx é…ç½®æ¸¬è©¦/é‡è¼‰å¤±æ•—ï¼Œè«‹æ‰‹å‹•æª¢æŸ¥ã€‚"
  }
  setup_nginx_pdf_logging

  echo "[ğŸš€] å•Ÿå‹• Python ä¸‹è¼‰å™¨..."
  TMP_PY="$(mktemp)"
  awk '/^# >>>PYTHON>>>$/{p=1;next} /^# <<<PYTHON<<</{p=0} p' "$0" > "$TMP_PY"
  exec python3 "$TMP_PY"
  echo "[!] ç„¡æ³•å•Ÿå‹• Python å­é€²ç¨‹ï¼Œè«‹æª¢æŸ¥ä¸Šæ–¹æ—¥èªŒã€‚" >&2
  exit 1
fi

# >>>PYTHON>>>
# -*- coding: utf-8 -*-
"""
SmartEdu æ‰¹é‡ä¸‹è¼‰å™¨ (polyglot v5.0)
- å¾ã€Œä¸‹è¼‰ç’°ç¯€ã€å¾¹åº•å»é‡ï¼šè¦ç¯„å‘½åï¼ˆå»æ‰ __hash/_hash/-æ—¥æœŸ/æ™‚é–“æˆ³ å°¾ç¶´ï¼‰ï¼Œä¸‹è¼‰å‰åŸºæ–¼ Content-Length + ç¾æœ‰æ–‡ä»¶é€²è¡Œåˆ¤æ–·ï¼Œå·²å­˜åœ¨ä¸”æ›´å¤§/ç›¸ç­‰å‰‡è·³éã€‚
- æ–·é»çºŒå‚³ï¼š.part æª”è‡ªå‹•çºŒä¸‹ï¼›ä¸‹è¼‰å®Œæˆå¾ŒåŸå­æ›¿æ›ã€‚
- æˆåŠŸå¾Œå³åˆ»æ›´æ–° index.json èˆ‡ index.htmlï¼ˆæœ€å¾Œä¸€ç‰ˆé é¢æ¨£å¼ï¼‰ï¼Œå­¸ç§‘å°èˆªé»æ“Šå¦‚ã€Œèªæ–‡ã€æœƒåŒæ™‚é¡¯ç¤ºåˆä¸­/é«˜ä¸­ç­‰æ‰€æœ‰å­¸æ®µå·²ä¸‹è¼‰æ•™æã€‚
- çœŸæ­£å»é‡è¼¸å‡ºåˆ°ç¶²é ï¼šåŒå­¸ç§‘ + åŒã€Œè¦ç¯„æ›¸åã€åªé¡¯ç¤ºä¸€æ¢ï¼Œä¿ç•™é«”ç©æ›´å¤§çš„ç‰ˆæœ¬ã€‚
- è‡ªå‹•é‡è©¦è¼ªï¼šæ•´è¼ªå¤±æ•—æ¸…å–®å¯å†è©¦ N è¼ªï¼ˆSMARTEDU_POST_RETRYï¼›é è¨­ 2ï¼›0=é—œé–‰ï¼‰ã€‚
"""
from __future__ import annotations

import os, re, json, asyncio, aiohttp, aiofiles, time, logging
import shutil
from logging import handlers
from pathlib import Path
from urllib.parse import quote
from typing import List, Dict, Any, Tuple, Optional
from collections import namedtuple
from tqdm import tqdm

# ---------------- åŸºæœ¬é…ç½® / å¸¸é‡ ----------------
Settings = namedtuple("Settings", [
    "PHASE","SUBJECTS","MATCH","IDS","OUT_DIR","WEB_DIR","ONLY_FAILED",
    "HCON","DCON","LIMIT","POST_RETRY"
])

PHASE_TAGS = {
    "å°å­¦": ["å°å­¦"],
    "åˆä¸­": ["åˆä¸­"],
    "é«˜ä¸­": ["é«˜ä¸­", "æ™®é€šé«˜ä¸­"],
    "ç‰¹æ®Šæ•™è‚²": ["ç‰¹æ®Šæ•™è‚²"],
    "å°å­¦54": ["å°å­¦ï¼ˆäº”â€¢å››å­¦åˆ¶ï¼‰", "å°å­¦ï¼ˆäº”Â·å››å­¦åˆ¶ï¼‰"],
    "åˆä¸­54": ["åˆä¸­ï¼ˆäº”â€¢å››å­¦åˆ¶ï¼‰", "åˆä¸­ï¼ˆäº”Â·å››å­¦åˆ¶ï¼‰"],
}

ORDER_SUBJ = ["è¯­æ–‡","æ•°å­¦","è‹±è¯­","ç‰©ç†","åŒ–å­¦","ç”Ÿç‰©","æ€æƒ³æ”¿æ²»","å†å²","åœ°ç†"]
SUBJ_RANK = {v:i for i,v in enumerate(ORDER_SUBJ)}
CLS = {"è¯­æ–‡":"yuwen","æ•°å­¦":"shuxue","è‹±è¯­":"yingyu","ç‰©ç†":"wuli","åŒ–å­¦":"huaxue","ç”Ÿç‰©":"shengwu","æ€æƒ³æ”¿æ²»":"zhengzhi","å†å²":"lishi","åœ°ç†":"dili"}
THEME = {
  "yuwen":   {"chip":"#C2410C","title":"#F59E0B","name":"#F8B76B","grad":"linear-gradient(135deg,#fb923c40,#fed7aa33)","border":"#fb923c","tint":"#2b1a12"},
  "shuxue":  {"chip":"#0D9488","title":"#34D399","name":"#7FE3C8","grad":"linear-gradient(135deg,#14b8a640,#99f6e433)","border":"#2dd4bf","tint":"#10201f"},
  "yingyu":  {"chip":"#2563EB","title":"#60A5FA","name":"#9EC5FF","grad":"linear-gradient(135deg,#3b82f640,#93c5fd33)","border":"#60a5fa","tint":"#121a2b"},
  "wuli":    {"chip":"#7C3AED","title":"#A78BFA","name":"#D2C3FF","grad":"linear-gradient(135deg,#8b5cf640,#c4b5fd33)","border":"#a78bfa","tint":"#191331"},
  "huaxue":  {"chip":"#16A34A","title":"#86EFAC","name":"#BFF5D2","grad":"linear-gradient(135deg,#22c55e40,#bbf7d033)","border":"#86efac","tint":"#0e1e14"},
  "shengwu": {"chip":"#059669","title":"#34D399","name":"#86EBCF","grad":"linear-gradient(135deg,#10b98140,#6ee7b733)","border":"#34d399","tint":"#0c1f1a"},
  "zhengzhi":{"chip":"#D97706","title":"#FBBF24","name":"#FFD683","grad":"linear-gradient(135deg,#f59e0b40,#fde68a33)","border":"#fbbf24","tint":"#261a08"},
  "lishi":   {"chip":"#EA580C","title":"#FB923C","name":"#FFC39C","grad":"linear-gradient(135deg,#f9731640,#fdba7433)","border":"#fb923c","tint":"#29170e"},
  "dili":    {"chip":"#0EA5E9","title":"#67E8F9","name":"#A8F4FE","grad":"linear-gradient(135deg,#06b6d440,#a5f3fc33)","border":"#67e8f9","tint":"#0d1f28"},
}

S_FILE_HOSTS = [
    "https://s-file-1.ykt.cbern.com.cn",
    "https://s-file-2.ykt.cbern.com.cn",
    "https://s-file-3.ykt.cbern.com.cn",
]
R_HOSTS = [
    "https://r1-ndr-oversea.ykt.cbern.com.cn",
    "https://r2-ndr-oversea.ykt.cbern.com.cn",
    "https://r3-ndr-oversea.ykt.cbern.com.cn",
    "https://r1-ndr.ykt.cbern.com.cn",
    "https://r2-ndr.ykt.cbern.com.cn",
    "https://r3-ndr.ykt.cbern.com.cn",
]
ENTRY_PATH = "/zxx/ndrs/resources/tch_material/version/data_version.json"
BASE_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_6_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118 Safari/537.36",
    "Accept": "application/json, text/plain, */*",
    "Accept-Language": "zh-CN,zh;q=0.9",
}

# ---------------- æ—¥èªŒ ----------------
LOGGER = logging.getLogger("smartedu")
def setup_logging(out_dir: Path):
    LOGGER.setLevel(logging.DEBUG)
    LOGGER.handlers.clear()
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(logging.Formatter("[%(asctime)s] %(levelname)s - %(message)s", datefmt="%H:%M:%S"))
    out_dir.mkdir(parents=True, exist_ok=True)
    fh = handlers.RotatingFileHandler(out_dir / "smartedu_download.log", maxBytes=10_000_000, backupCount=2, encoding="utf-8")
    fh.setLevel(logging.DEBUG)
    fh.setFormatter(logging.Formatter("[%(asctime)s] %(levelname)s %(name)s:%(lineno)d - %(message)s", datefmt="%Y-%m-%d %H:%M:%S"))
    LOGGER.addHandler(ch); LOGGER.addHandler(fh)

# ---------------- å·¥å…· ----------------
def esc(s: str) -> str:
    return (s or "").replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")

def have_pdf_head(p: Path) -> bool:
    try:
        if not p.exists() or p.stat().st_size < 100*1024: return False
        with open(p,'rb') as f: return f.read(5) == b'%PDF-'
    except Exception: return False

HEX = r"[0-9a-fA-F]{6,}"
TS  = r"\d{10,14}"
DATE= r"\d{8}"
TAIL_PAT = re.compile(rf"(?:__|_|-)(?:{HEX}|{TS}|{DATE})$", re.IGNORECASE)
PAREN_HASH_TS = re.compile(rf"\((?:{HEX}|{TS}|{DATE})\)$", re.IGNORECASE)

def canon_title(s: str) -> str:
    s = (s or "").strip().replace("ï¼ˆ","(").replace("ï¼‰",")")
    s = PAREN_HASH_TS.sub("", s)
    while True:
        t = TAIL_PAT.sub("", s)
        if t == s: break
        s = t
    return re.sub(r"\s+", " ", s) or "æœªå‘½åæ•™æ"

def canon_filename(name_or_title: str) -> str:
    base = canon_title(name_or_title)
    if not base.lower().endswith(".pdf"): base += ".pdf"
    safe = re.sub(r'[\\/:*?"<>|]', "_", base)
    safe = re.sub(r"\s+", " ", safe)
    return safe

def logic_key(subject: str, name_or_title: str) -> str:
    key = canon_title(name_or_title)
    key = re.sub(r"\s+", "", key)
    key = key.replace("ï¼ˆ","(").replace("ï¼‰",")")
    return f"{subject}::{key}"

def load_settings_from_env() -> Settings:
    out_env = os.getenv("SMARTEDU_OUT_DIR")
    out_dir = Path(os.path.expanduser(out_env)) if out_env else Path.cwd() / "smartedu_textbooks"
    pr_raw = os.getenv("SMARTEDU_POST_RETRY", "2").strip()
    try: pr = max(0, min(5, int(pr_raw)))
    except ValueError: pr = 2
    web_env = os.getenv("SMARTEDU_WEB_DIR","").strip()
    web_dir = Path(os.path.expanduser(web_env)) if web_env else out_dir
    return Settings(
        PHASE=os.getenv("SMARTEDU_PHASE","é«˜ä¸­"),
        SUBJECTS=[s.strip().replace(" ","") for s in os.getenv("SMARTEDU_SUBJ","è¯­æ–‡,æ•°å­¦,è‹±è¯­,æ€æƒ³æ”¿æ²»,å†å²,åœ°ç†,ç‰©ç†,åŒ–å­¦,ç”Ÿç‰©").split(",") if s.strip()],
        MATCH=os.getenv("SMARTEDU_MATCH","").strip(),
        IDS=[s.strip() for s in os.getenv("SMARTEDU_IDS","").split(",") if s.strip()],
        OUT_DIR=out_dir,
        WEB_DIR=web_dir,
        ONLY_FAILED=os.getenv("SMARTEDU_ONLY_FAILED","0")=="1",
        HCON=int(os.getenv("SMARTEDU_HCON","12")),
        DCON=int(os.getenv("SMARTEDU_DCON","5")),
        LIMIT=int(v) if (v:=os.getenv("SMARTEDU_LIMIT","").strip()).isdigit() else None,
        POST_RETRY=pr,
    )

def build_referer(book_id: str) -> str:
    return ("https://basic.smartedu.cn/tchMaterial/detail"
            f"?contentType=assets_document&amp;contentId={book_id}"
            "&amp;catalogType=tchMaterial&amp;subCatalog=tchMaterial")

# ---------------- é ç«¯è³‡æºæŠ“å– ----------------
async def get_json(session: aiohttp.ClientSession, url: str) -> Optional[Dict | List]:
    for i in range(3):
        try:
            async with session.get(url, headers=BASE_HEADERS, timeout=30) as resp:
                txt = await resp.text()
                if resp.status == 200 and ("json" in (resp.headers.get("Content-Type","") or "").lower() or txt[:1] in "[{"):
                    return json.loads(txt)
        except (aiohttp.ClientError, asyncio.TimeoutError):
            await asyncio.sleep(1.2 * (i+1))
    return None

async def get_data_urls(session: aiohttp.ClientSession) -> List[str]:
    for base in S_FILE_HOSTS:
        js = await get_json(session, base + ENTRY_PATH)
        if isinstance(js, dict):
            field = js.get("urls") or js.get("url")
            urls: List[str] = []
            if isinstance(field, str):
                urls = [u.strip() for u in field.split(",") if u.strip()]
            elif isinstance(field, list):
                urls = [str(u).strip() for u in field if str(u).strip()]
            if urls: return urls
    return []

def book_tags(book: Dict[str,Any]) -> List[str]:
    return [t.get("tag_name","") for t in (book.get("tag_list") or [])]

def match_phase_subject_keyword(book: Dict[str, Any], st: Settings) -> bool:
    tags = book_tags(book)
    wants = PHASE_TAGS.get(st.PHASE, [])
    if wants and not any(any(w in t for w in wants) for t in tags): return False
    if st.SUBJECTS and not any(any(s in t for s in st.SUBJECTS) for t in tags): return False
    if st.MATCH:
        title = (book.get("title") or (book.get("global_title") or {}).get("zh-CN") or "").lower()
        if not all(k.lower() in title for k in st.MATCH.split()): return False
    return True

def derive_filename(item: dict, book_id: str) -> Optional[str]:
    stor = item.get("ti_storage") or (item.get("ti_storages") or [None])[0]
    if isinstance(stor, str) and ".pkg/" in stor:
        tail = stor.replace("cs_path:${ref-path}", "").lstrip("/")
        fname = tail.split(".pkg/", 1)[-1]
        base = fname.split("/")[-1] if fname else None
        if base: return base
    fname = item.get("ti_filename")
    if isinstance(fname, str) and fname.lower().endswith(".pdf"):
        return fname.split("/")[-1]
    title = item.get("ti_title") or item.get("title")
    if isinstance(title, str) and title.strip():
        t = title.strip()
        if not t.lower().endswith(".pdf"): t += ".pdf"
        return t
    return None

def candidates_from_detail(book_id: str, items: List[dict]) -> List[str]:
    urls=[]
    for it in items:
        if (it.get("ti_format") or it.get("format") or "").lower() != "pdf": continue
        fname = derive_filename(it, book_id)
        if not fname: continue
        raw = f"esp/assets/{book_id}.pkg/{fname}"
        enc = f"esp/assets/{book_id}.pkg/{quote(fname)}"
        for host in R_HOSTS:
            urls.append(f"{host}/edu_product/{raw}")
            urls.append(f"{host}/edu_product/{enc}")
    for host in R_HOSTS:
        urls.append(f"{host}/edu_product/esp/assets/{book_id}.pkg/pdf.pdf")
    dedup=[]
    seen=set()
    for u in urls:
        if u not in seen: seen.add(u); dedup.append(u)
    return dedup

async def resolve_candidates(session: aiohttp.ClientSession, book_id: str) -> List[str]:
    for base in S_FILE_HOSTS:
        js = await get_json(session, f"{base}/zxx/ndrv2/resources/tch_material/details/{book_id}.json")
        if isinstance(js, dict):
            items = js.get("ti_items") or []
            if items: return candidates_from_detail(book_id, items)
    return [f"{h}/edu_product/esp/assets/{book_id}.pkg/pdf.pdf" for h in R_HOSTS]

async def probe_url(session: aiohttp.ClientSession, url: str, referer: str) -> Tuple[bool, Optional[int]]:
    headers = {**BASE_HEADERS, "Referer": referer}
    try:
        async with session.head(url, headers=headers, timeout=20, allow_redirects=True) as r:
            if r.status == 200:
                cl = r.headers.get("Content-Length")
                ct = (r.headers.get("Content-Type","") or "").lower()
                if ("pdf" in ct or url.lower().endswith(".pdf")) and (cl is None or int(cl) > 50*1024):
                    return True, int(cl) if cl else None
    except Exception:
        pass
    # fallback range GET
    try:
        headers["Range"] = "bytes=0-1"
        async with session.get(url, headers=headers, timeout=20, allow_redirects=True) as r:
            if r.status in (200,206):
                cl = r.headers.get("Content-Length")
                ct = (r.headers.get("Content-Type","") or "").lower()
                return ("pdf" in ct or url.lower().endswith(".pdf")), int(cl) if cl else None
    except Exception:
        return False, None
    return False, None

# ---------------- ä¸‹è¼‰èˆ‡å»é‡ ----------------
def existing_index(out_dir: Path) -> List[Dict[str,Any]]:
    idx = out_dir / "index.json"
    if idx.exists():
        try: return json.loads(idx.read_text("utf-8"))
        except Exception: return []
    return []

def build_existing_map(out_dir: Path) -> Dict[str, Dict[str,Any]]:
    m={}
    for it in existing_index(out_dir):
        subj = it.get("subject") or "ç¶œåˆ"
        key  = logic_key(subj, it.get("title") or Path(it.get("path","")).stem)
        m[key] = it
    # åŒæ™‚å¾ç£ç¢Ÿæƒæè£œå…¨ï¼ˆé¿å…æ‰‹å·¥ç§»å‹•å°è‡´ç´¢å¼•æ¼ï¼‰
    for p in out_dir.rglob("*.pdf"):
        rel = p.relative_to(out_dir).as_posix()
        subj_guess = next((s for s in ORDER_SUBJ if f"/{s}/" in ("/"+rel+"/")), "ç¶œåˆ")
        key = logic_key(subj_guess, p.stem)
        if key not in m:
            m[key] = {"title": canon_title(p.stem), "subject": subj_guess, "phase": "", "path": str(p), "size": p.stat().st_size}
    return m

# --- åˆä½µç¾æœ‰æ–‡ä»¶æ˜ å°„ï¼šprimary è¦†è“‹ secondaryï¼Œä¿ç•™æ›´å¤§è€… ---
def merge_maps(primary: Dict[str,Any], secondary: Dict[str,Any]) -> Dict[str,Any]:
    """æŒ‰ key åˆä½µï¼Œä¿ç•™ size æ›´å¤§è€…ï¼›ç›¸ç­‰æ™‚ä¿ç•™è·¯å¾‘æ›´çŸ­è€…ã€‚"""
    out = dict(secondary)
    for k,v in primary.items():
        ov = out.get(k)
        if not ov:
            out[k]=v; continue
        try:
            sz1 = int(v.get("size") or 0)
            sz2 = int(ov.get("size") or 0)
        except Exception:
            sz1 = int(v.get("size") or 0); sz2 = int(ov.get("size") or 0)
        if (sz1 > sz2) or (sz1==sz2 and len(str(v.get("path",""))) < len(str(ov.get("path","")))):
            out[k]=v
    return out

def mirror_to_web_dir(out_dir: Path, web_dir: Path, combined: Dict[str,Any]) -> None:
    """æŠŠ OUT_DIR çš„æ–‡ä»¶é¡åƒåˆ° WEB_DIRï¼šè‹¥ç›®æ¨™ä¸å­˜åœ¨æˆ–æ›´å°å‰‡è¦†è“‹ï¼Œä¿ç•™ç›®éŒ„çµæ§‹ã€‚"""
    for it in combined.values():
        p = Path(it.get("path",""))
        src = (out_dir / p) if not p.is_absolute() else p
        if not src.exists(): 
            continue
        try:
            rel = src.relative_to(out_dir)
        except Exception:
            continue
        dst = web_dir / rel
        try:
            dst.parent.mkdir(parents=True, exist_ok=True)
            if (not dst.exists()) or (dst.stat().st_size < src.stat().st_size):
                shutil.copy2(src, dst)
        except Exception as e:
            LOGGER.warning("é¡åƒåˆ°ç¶²é ç›®éŒ„å¤±æ•—: %s -> %s (%s)", src, dst, e)

async def download_pdf(session: aiohttp.ClientSession, url: str, dest: Path, referer: str) -> bool:
    if have_pdf_head(dest):
        LOGGER.info("å·²å­˜åœ¨æœ‰æ•ˆ PDFï¼Œè·³é: %s", dest.name); return True
    tmp = dest.with_suffix(".part")
    start = tmp.stat().st_size if tmp.exists() else 0
    headers = {**BASE_HEADERS, "Referer": referer}
    if start>0: headers["Range"]=f"bytes={start}-"
    for attempt in range(3):
        try:
            async with session.get(url, headers=headers, timeout=180) as r:
                if r.status not in (200,206):
                    LOGGER.debug("ä¸‹è¼‰ HTTP %s: %s", r.status, url); await asyncio.sleep(2*(attempt+1)); continue
                dest.parent.mkdir(parents=True, exist_ok=True)
                mode = "ab" if (start>0 and r.status==206) else "wb"
                async with aiofiles.open(tmp, mode) as f:
                    async for chunk in r.content.iter_chunked(1<<14):
                        await f.write(chunk)
                tmp.replace(dest)
                return have_pdf_head(dest)
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            LOGGER.debug("ä¸‹è¼‰ç•°å¸¸ (%d/3) %s", attempt+1, e); await asyncio.sleep(2.5*(attempt+1))
    LOGGER.warning("ä¸‹è¼‰å¤±æ•—: %s", url)
    return False

# ---------------- HTML ç”Ÿæˆï¼ˆæœ€çµ‚ç‰ˆæ¨£å¼ï¼Œç„¡ Template ä½”ä½é¢¨éšªï¼‰ ----------------
def render_html(out_dir: Path, items: List[Dict[str,Any]]):
    # â€”â€” åˆä½µ & çœŸæ­£å»é‡ï¼ˆåŒå­¸ç§‘ + è¦ç¯„æ›¸åï¼Œä¿ç•™æ›´å¤§è€…ï¼‰ â€”â€”
    collected={}
    for it in items:
        subj = it.get("subject") or "ç¶œåˆ"
        title= canon_title(it.get("title") or Path(it.get("path","")).stem)
        path = Path(it.get("path",""))
        if not (out_dir/path).exists(): 
            # å…¼å®¹å­˜å„²ç‚ºçµ•å°è·¯å¾‘
            if path.exists(): pass
            else: continue
        size = (out_dir/path).stat().st_size if (out_dir/path).exists() else path.stat().st_size
        key  = logic_key(subj, title)
        old  = collected.get(key)
        if (not old) or (size > old["_size"]) or (size==old["_size"] and len(str(path))<len(old["_rel"])):
            collected[key]={"_rel":str(path), "_size":size, "_disp":title, "subject":subj, "_fname":Path(path).name}

    # â€”â€” åˆ†çµ„èˆ‡æ’åº â€”â€”
    by={}
    for v in collected.values():
        by.setdefault(v["subject"], []).append(v)
    subjects = sorted(by.keys(), key=lambda s:(SUBJ_RANK.get(s,999), s))
    for s in subjects:
        by[s].sort(key=lambda v: v["_disp"])

    # â€”â€” chips â€”â€” 
    def anchor(s:str): return f"subj-{s.replace(' ','-')}"
    chips = ['<a class="chip chip--all" data-all="1" href="#">å…¨éƒ¨</a>']
    for s in ORDER_SUBJ:
        if s in by:
            chips.append(f'<a class="chip chip--{CLS.get(s,"generic")}" href="#{esc(anchor(s))}" data-subj="{esc(s)}">{esc(s)}</a>')
    chips_html="".join(chips)

    # â€”â€” ç§‘ç›® CSS â€”â€” 
    subject_css=[]
    for subj,cls in CLS.items():
        th = THEME[cls]
        subject_css.append(
f""".chip--{cls}{{background:{th['chip']};border-color:#1e2833;color:#fff;}}
.section--{cls} .name{{color:{th['name']};}}
.section--{cls} > h2{{color:{th['title']};}}
""")
    subject_css="".join(subject_css)

    # â€”â€” sections â€”â€” 
    sections=[]
    for s in subjects:
        cls = CLS.get(s,"generic")
        th  = THEME.get(cls, THEME["yingyu"])
        cards=[]
        for v in by[s]:
            rel = v["_rel"].replace(os.sep,"/")
            href = quote(rel, safe="/")
            size_mb = f"{(v['_size']/1024/1024):.1f}MB"
            cards.append(
                f'<li class="card" data-title="{esc(v["_disp"])}">'
                f'  <a class="card-link" href="{href}" target="_blank" download title="{esc(v["_fname"])}">'
                f'    <div class="thumb" aria-hidden="true" style="background:{th["grad"]};border-color:{th["border"]}"><span>ğŸ“„</span></div>'
                f'    <div class="meta">'
                f'      <div class="name">{esc(v["_disp"])}</div>'
                f'      <div class="filesize">{esc(size_mb)}</div>'
                f'      <div class="subj subj--{cls}">{esc(s)}</div>'
                f'    </div>'
                f'  </a>'
                f'</li>'
            )
        sections.append(
            f'<section id="{esc(anchor(s))}" class="section section--{cls}" style="background:{THEME[cls]["tint"]}33">'
            f'  <h2>{esc(s)}</h2>'
            f'  <ul class="grid">{"".join(cards)}</ul>'
            f'</section>'
        )

    HTML_TMPL = """<!doctype html>
<html lang="zh-CN">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>BDFZ- Suen æ•™æåº«</title>
<link rel="icon" href="https://img.bdfz.net/20250503004.webp" type="image/jpeg">
<link rel="icon" href="/favicon.ico" sizes="any">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<style>
  :root { --bg:#0b0d10; --fg:#e6edf3; --muted:#9aa4ad; --line:#161f29; --card:#0f141a; --accent:#6ab7ff; }
  * { box-sizing:border-box; }
  body { margin:0; font:14px/1.6 -apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,'PingFang SC','Noto Sans CJK SC','Hiragino Sans GB','Microsoft YaHei',sans-serif; background:var(--bg); color:var(--fg); }
  header { position:sticky; top:0; z-index:10; padding:10px 12px 10px; border-bottom:1px solid #11161c; background:rgba(11,13,16,.92); backdrop-filter: blur(8px); }
  .container { max-width: 1280px; margin: 0 auto; }
  .center { display:flex; justify-content:center; align-items:center; flex-wrap:wrap; gap:8px; }
  .chips { padding:4px 0 6px; }
  .chip { display:inline-block; padding:6px 12px; border-radius:999px; white-space:nowrap; color:#fff; text-decoration:none; border:1px solid #1e2833; }
  .chip--all { background:#374151; }
  .toolbar { margin-top:6px; gap:8px; }
  .btn, .link { color:#cbd5e1; text-decoration:none; background:#0f141a; border:1px solid #17212b; border-radius:8px; padding:6px 10px; }
  .btn:hover, .link:hover { border-color:#2a3644; color:var(--accent); cursor:pointer;}
  input[type="search"] { background:#0f141a; color:#e6edf3; border:1px solid #17212b; border-radius:8px; padding:6px 10px; min-width:200px; outline:none; }
  main { padding:18px 12px; }
  .page { max-width:1280px; margin:0 auto; }
  section { margin:18px 0 28px; border-radius:14px; padding:8px 10px 12px; }
  section>h2 { font-size:16px; margin:6px 0 12px; text-align:center; scroll-margin-top: 120px; color:#dbe7f3; }
  .grid { list-style:none; padding:0; margin:0 auto; display:grid; grid-template-columns:repeat(auto-fill,minmax(280px,1fr)); gap:12px; }
  .card { background:var(--card); border:1px solid var(--line); border-radius:12px; }
  .card-link { display:flex; align-items:center; gap:12px; padding:12px; color:inherit; text-decoration:none; }
  .thumb { width:40px; height:52px; border-radius:8px; border:1px solid #24425e; display:flex; align-items:center; justify-content:center; flex:0 0 auto; }
  .thumb span { font-size:16px; }
  .meta { min-width:0; display:flex; flex-direction:column; gap:6px; }
  .name { font-size:14px; white-space:normal; word-break:break-all; overflow:visible; }
  .filesize { font-size:12px; color:#9aa4ad; }
  .subj { color:#9aa4ad; font-size:12px; }
  /* SUBJECT_CSS */
  @media (max-width: 720px) {
    .container { padding:0 6px; }
    .page { padding:0 4px; }
    input[type="search"] { min-width: 56vw; }
    .grid { grid-template-columns:repeat(auto-fill,minmax(220px,1fr)); gap:10px; }
    .thumb { width:36px; height:48px; }
    section>h2 { scroll-margin-top: 150px; }
  }
  .toast { position:fixed; right:16px; bottom:16px; background:#0f141a; border:1px solid #1f2a37; color:#e6edf3; padding:10px 12px; border-radius:10px; box-shadow:0 10px 30px rgba(0,0,0,.4); display:none; max-width:70vw; }
</style>
</head>
<body>
  <header>
    <div class="container">
      <div class="center chips">/*CHIPS*/</div>
      <div class="center toolbar">
        <input id="kw" type="search" placeholder="é—œéµè©ç¯©é¸ï¼ˆæ›¸åï¼‰">
        <a class="link" href="https://bdfz.net/posts/jks/" target="_blank" rel="noopener">About</a>
      </div>
    </div>
  </header>
  <main>
    <div class="page">
      /*SECTIONS*/
    </div>
  </main>
  <div id="toast" class="toast"></div>
<script>
(function(){
  const kw=document.getElementById('kw');
  function apply(){
    const k=kw.value.trim().toLowerCase();
    for(const li of document.querySelectorAll('.card')){
      const title=(li.getAttribute('data-title')||'').toLowerCase();
      li.style.display=(!k||title.includes(k))?'block':'none';
    }
  }
  kw.addEventListener('input', apply);

  // å­¦ç§‘ chipsï¼šé»æ“Šå¾Œåªé¡¯ç¤ºè©²å­¸ç§‘ï¼Œæ»¾å‹•åˆ°æ¨™é¡Œï¼›â€œå…¨éƒ¨â€æ¢å¾©
  const chips=document.querySelectorAll('.chip');
  chips.forEach(ch=>{
    ch.addEventListener('click', (e)=>{
      const all = ch.dataset.all === '1';
      if(all){
        e.preventDefault();
        document.querySelectorAll('section').forEach(sec=>sec.style.display='block');
        window.scrollTo({top:0, behavior:'smooth'}); return;
      }
      const subj = ch.dataset.subj; if(!subj) return;
      e.preventDefault();
      const id = 'subj-' + subj.replace(/\\s+/g,'-');
      const sec = document.getElementById(id);
      if(sec){
        document.querySelectorAll('section').forEach(s=>s.style.display='none');
        sec.style.display='block';
        sec.querySelector('h2')?.scrollIntoView({behavior:'smooth', block:'start'});
      }
      chips.forEach(x=>x.style.outline='none');
      ch.style.outline='2px solid rgba(255,255,255,.25)'; ch.style.outlineOffset='2px';
    });
  });
})();
</script>
</body>
</html>
"""
    chips_html = chips_html
    sections_html = "".join(sections)
    html = (HTML_TMPL
            .replace("/*SUBJECT_CSS*/", subject_css)
            .replace("/*CHIPS*/", chips_html)
            .replace("/*SECTIONS*/", sections_html))
    (out_dir/"index.html").write_text(html, "utf-8")

# ---------------- ä¸»æµç¨‹ ----------------
async def resolve_all_books(session: aiohttp.ClientSession, st: Settings) -> List[Dict[str,Any]]:
    # æŒ‡å®š IDS ç›´é”
    if st.IDS:
        return [{"id": i, "title": i, "tag_list":[{"tag_name": st.PHASE}]} for i in st.IDS]

    LOGGER.info("ğŸ” è®€å–é ç¨‹ç´¢å¼•...")
    urls = await get_data_urls(session)
    if not urls:
        LOGGER.error("ç„¡æ³•ç²å– data_version.json çš„ urlsã€‚"); return []
    books: List[Dict[str,Any]] = []
    for url in urls:
        js = await get_json(session, url)
        if isinstance(js, list): books.extend(js)
    books = [b for b in books if match_phase_subject_keyword(b, st)]
    if st.LIMIT: books = books[:st.LIMIT]
    LOGGER.info("ç›®æ¨™æ¢ç›®: %d", len(books))
    return books

async def main():
    st = load_settings_from_env()
    out_dir: Path = st.OUT_DIR
    web_dir: Path = st.WEB_DIR
    setup_logging(out_dir)
    LOGGER.info("ğŸ“ ä¸‹è¼‰ç›®éŒ„: %s", out_dir)
    LOGGER.info("ğŸŒ ç¶²é ç›®éŒ„: %s", web_dir)
    LOGGER.info("éšæ®µ=%s | å­¸ç§‘=%s | åŒ¹é…='%s' | åªé‡è©¦å¤±æ•—=%s | è‡ªå‹•é‡è©¦è¼ª=%d",
                st.PHASE, ",".join(st.SUBJECTS), st.MATCH, st.ONLY_FAILED, st.POST_RETRY)

    timeout = aiohttp.ClientTimeout(total=None, sock_connect=20, sock_read=180)
    async with aiohttp.ClientSession(timeout=timeout) as session:
        books = await resolve_all_books(session, st)
        if not books:
            LOGGER.warning("æ²’æœ‰åŒ¹é…çš„æ¢ç›®ã€‚ä»å°‡åˆ·æ–°ç¶²é ç´¢å¼•ã€‚")

        # æ§‹å»ºç¾æœ‰æ–‡ä»¶æ˜ å°„ï¼ˆåˆä½µ OUT_DIR èˆ‡ WEB_DIRï¼Œä¿ç•™æ›´å¤§è€…ï¼‰
        exist_map = merge_maps(build_existing_map(out_dir), build_existing_map(web_dir))

        # æº–å‚™ä¸‹è¼‰éšŠåˆ—ï¼ˆè§£æç›´éˆï¼‰
        sem = asyncio.Semaphore(st.HCON)
        queue: List[Tuple[str,str,str,str,Optional[int]]] = []  # (bid,title,subj,url,remote_len)
        pbar = tqdm(total=len(books), desc="è§£æç›´éˆ", unit="æœ¬")
        for book in books:
            async with sem:
                bid = book.get("id") or book.get("content_id")
                title = canon_title(book.get("title") or (book.get("global_title") or {}).get("zh-CN") or bid)
                subj  = next((s for s in st.SUBJECTS if any(s in t for t in book_tags(book))), "ç¶œåˆ")
                if not bid: pbar.update(1); continue
                ref = build_referer(bid)
                urls = await resolve_candidates(session, bid)
                remote_len = None
                chosen = None
                # å–ç¬¬ä¸€å€‹æ¢æ¸¬å¯ç”¨çš„ç›´éˆï¼ŒåŒæ™‚ç²å– Content-Length
                for u in urls:
                    ok, rlen = await probe_url(session, u, ref)
                    if ok:
                        chosen=u; remote_len=rlen; break
                if not chosen:
                    pbar.update(1); continue
                queue.append((bid, title, subj, chosen, remote_len))
                pbar.update(1)
        pbar.close()

        # ä¸‹è¼‰ï¼ˆæ”¯æŒæ–·é»èˆ‡è·³éï¼‰ï¼ŒæŒ‰ DCON æ§åˆ¶ä¸¦ç™¼
        async def worker(items):
            for bid, title, subj, url, rlen in items:
                # ç›®éŒ„ï¼šout/å­¸æ®µ/å­¸ç§‘/
                dest_dir = out_dir / st.PHASE / subj
                dest_dir.mkdir(parents=True, exist_ok=True)
                dest = dest_dir / canon_filename(title)
                key  = logic_key(subj, title)

                # è‹¥å·²æœ‰ç›¸åŒ key çš„æ–‡ä»¶ï¼ˆä»»ä½•å­¸æ®µï¼‰ï¼Œä¸”æª”æ¡ˆæœ‰æ•ˆã€å¤§å° >= é ç«¯ï¼ˆè‹¥å·²çŸ¥ï¼‰ï¼Œè·³é
                exist = exist_map.get(key)
                if exist:
                    p = Path(exist.get("path",""))
                    p = (out_dir/p) if not p.is_absolute() else p
                    if p.exists() and have_pdf_head(p):
                        if rlen is None or p.stat().st_size >= rlen:
                            LOGGER.info("è·³éï¼ˆå·²å­˜åœ¨æ›´å¤§/ç›¸ç­‰ï¼‰: %s", title); 
                            continue

                # è‹¥ç›®æ¨™è·¯å¾‘å·²æœ‰æœ‰æ•ˆ PDFï¼Œäº¦è·³é
                if have_pdf_head(dest):
                    LOGGER.info("è·³éï¼ˆæœ¬åœ°å·²å®Œæ•´ï¼‰: %s", dest.name); 
                    continue

                ok = await download_pdf(session, url, dest, build_referer(bid))
                if ok:
                    exist_map[key] = {"title": title, "subject": subj, "phase": st.PHASE, "path": str(dest.relative_to(out_dir)), "size": dest.stat().st_size}
                else:
                    failures.append({"id": bid, "title": title, "subject": subj, "phase": st.PHASE, "url": url})

        # æ‹†åˆ†çµ¦ DCON å€‹ worker
        failures: List[Dict[str,Any]] = []
        if queue:
            chunks = [queue[i::max(1,st.DCON)] for i in range(max(1,st.DCON))]
            tasks = [asyncio.create_task(worker(ch)) for ch in chunks]
            await asyncio.gather(*tasks)

        # è‡ªå‹•é‡è©¦è¼ªï¼šåªé‡å°å¤±æ•—æ¸…å–®ï¼Œå†è·‘ st.POST_RETRY è¼ª
        for round_i in range(st.POST_RETRY):
            if not failures: break
            LOGGER.info("â™»ï¸ è‡ªå‹•é‡è©¦è¼ª %d / %dï¼Œå‰©é¤˜ %d æœ¬", round_i+1, st.POST_RETRY, len(failures))
            retrying = failures; failures=[]
            # é‡æ–°è§£æ+ä¸‹è¼‰
            q2=[]
            for f in retrying:
                bid=f["id"]; title=f["title"]; subj=f["subject"]; ref=build_referer(bid)
                urls = await resolve_candidates(session, bid)
                chosen=None; rlen=None
                for u in urls:
                    ok, rlen = await probe_url(session, u, ref)
                    if ok: chosen=u; break
                if chosen: q2.append((bid,title,subj,chosen,rlen))
            if q2:
                chunks = [q2[i::max(1,st.DCON)] for i in range(max(1,st.DCON))]
                tasks = [asyncio.create_task(worker(ch)) for ch in chunks]
                await asyncio.gather(*tasks)

        # â€”â€” æŠŠ OUT_DIR çš„æ–°å¢/æ›´å¤§æª”é¡åƒåˆ° WEB_DIR â€”â€” 
        mirror_to_web_dir(out_dir, web_dir, exist_map)

        # â€”â€” ä»¥ WEB_DIR ç‚ºæº–é‡å»º index.json èˆ‡é é¢ â€”â€” 
        web_map = build_existing_map(web_dir)
        items = []
        for k,it in web_map.items():
            p = Path(it["path"])
            abs_p = (web_dir/p) if not p.is_absolute() else p
            if abs_p.exists() and have_pdf_head(abs_p):
                it["size"] = abs_p.stat().st_size
                it["title"]= canon_title(it.get("title") or p.stem)
                try:
                    rel = abs_p.relative_to(web_dir).as_posix()
                except Exception:
                    rel = str(abs_p)
                it["path"] = rel
                items.append(it)
        (web_dir/"index.json").write_text(json.dumps(items, ensure_ascii=False, indent=2), "utf-8")

        # å¤±æ•—æ¸…å–®
        if failures:
            (out_dir/"failed.json").write_text(json.dumps(failures, ensure_ascii=False, indent=2), "utf-8")
            LOGGER.warning("ä»å¤±æ•— %d æœ¬ï¼›è©³è¦‹ failed.jsonï¼Œå¯ç”¨ -R åƒ…é‡è©¦å¤±æ•—ã€‚", len(failures))
        else:
            try: (out_dir/"failed.json").unlink()
            except FileNotFoundError: pass
            LOGGER.info("âœ… æœ¬è¼ªå…¨éƒ¨æˆåŠŸæˆ–å·²å­˜åœ¨ï¼ˆå»é‡è·³éï¼‰ã€‚")

        # ç”Ÿæˆæœ€çµ‚ç‰ˆç¶²é ï¼ˆå¯«å…¥ WEB_DIRï¼‰
        render_html(web_dir, items)
        LOGGER.info("ğŸ§­ å·²æ›´æ–° %s", (web_dir/"index.html"))

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass