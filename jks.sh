# =============================================================================
# smartedu_fetch_all.sh  â€”â€”  Shell + Python polyglot (v1.0)
# -----------------------------------------------------------------------------
# æ–°å¢ï¼š
#  â€¢ æ•´è¼ªçµæŸå¾Œè‡ªå‹•é‡è©¦ï¼ˆé è¨­ 2 è¼ªï¼Œå¯ç”¨ -T N èª¿æ•´ï¼›0 è¡¨ç¤ºé—œé–‰ï¼‰ã€‚
#  â€¢ æ¯è¼ªé‡è©¦åƒ…é‡å°ä¸Šè¼ªå¤±æ•—æ¸…å–®ï¼ŒæˆåŠŸå³å¯«å› index.jsonï¼Œä»å¤±æ•—ä¿ç•™åˆ° failed.jsonã€‚
#  â€¢ æœ€çµ‚è‹¥ä»æœ‰å¤±æ•—ï¼Œæ¸…æ™°æç¤ºç”¨æˆ¶å¯å†æ¬¡é‹è¡Œæˆ–ç”¨ -R åƒ…é‡è©¦å¤±æ•—ã€‚
# å…¶ä»–ï¼š
#  â€¢ ä¿æŒ v4.2 çš„ç©©å®šæ€§ï¼šå¤šä¸»æ©Ÿç´¢å¼•/è©³æƒ…ã€Referer å®Œæ•´ã€HEAD/Range æ¢æ¸¬ã€æ–·é»çºŒå‚³ã€å»é‡ã€è©³ç›¡æ—¥èªŒã€‚
# -----------------------------------------------------------------------------
# ç”¨æ³•ï¼š
#   bash smartedu_fetch_all.sh -p é«˜ä¸­
#   bash smartedu_fetch_all.sh -p é«˜ä¸­ -s è¯­æ–‡,æ•°å­¦ -m "å¿…ä¿® ç¬¬ä¸€å†Œ" -T 3
#   bash smartedu_fetch_all.sh -R -o ./output_dir
# =============================================================================

# ---- å¦‚æœä»¥ python æ–¹å¼èª¿ç”¨ï¼Œç›´æ¥è·³é Shell éƒ¨åˆ† ----
if [ -n "${PYTHON_EXEC:-}" ]; then
  :
else
  set -euo pipefail

  # é è¨­åƒæ•¸
  PHASE="é«˜ä¸­"
  SUBJECTS="è¯­æ–‡,æ•°å­¦,è‹±è¯­,æ€æƒ³æ”¿æ²»,å†å²,åœ°ç†,ç‰©ç†,åŒ–å­¦,ç”Ÿç‰©"
  MATCH=""
  IDS=""
  OUT_DIR=""
  ONLY_FAILED="0"
  HCON=12
  DCON=5
  LIMIT=""
  POST_RETRY=2

  usage() {
    cat >&2 <<'USAGE'
ç”¨æ³•:
  bash smartedu_fetch_all.sh [é¸é …]

é¸é …:
  -p PHASE         æ•™è‚²éšæ®µï¼šå°å­¦|åˆä¸­|é«˜ä¸­|ç‰¹æ®Šæ•™è‚²|å°å­¦54|åˆä¸­54    (é è¨­: é«˜ä¸­)
  -s SUBJECTS      å­¸ç§‘é€—è™Ÿåˆ†éš” (é è¨­: è¯­æ–‡,æ•°å­¦,è‹±è¯­,æ€æƒ³æ”¿æ²»,å†å²,åœ°ç†,ç‰©ç†,åŒ–å­¦,ç”Ÿç‰©)
  -m KEYWORD       æ›¸åé—œéµè©ï¼ˆå­ä¸²åŒ¹é…ï¼Œå¯å¤šè©ä»¥ç©ºæ ¼åˆ†éš”ï¼‰
  -i IDS           æŒ‡å®š contentId åˆ—è¡¨ï¼Œé€—è™Ÿåˆ†éš”ï¼ˆè·³éç´¢å¼•éæ¿¾ï¼Œç›´é”ï¼‰
  -o OUT_DIR       è‡ªå®šç¾©è¼¸å‡ºç›®éŒ„ï¼ˆé è¨­: ./smartedu_textbooksï¼‰
  -R               åƒ…é‡è©¦ä¸Šæ¬¡å¤±æ•—ï¼ˆè®€ OUT_DIR/failed.jsonï¼‰
  -c N             è§£æç›´éˆä¸¦ç™¼ (HEAD æª¢æŸ¥)ï¼Œé è¨­ 12
  -d N             ä¸‹è¼‰ä¸¦ç™¼ï¼Œé è¨­ 5
  -n N             åªè™•ç†å‰ N æœ¬ï¼ˆèª¿è©¦ç”¨ï¼‰
  -T N             æ•´è¼ªçµæŸå¾Œè‡ªå‹•é‡è©¦ N è¼ªï¼ˆé è¨­ 2ï¼›0=é—œé–‰ï¼‰
  -h               é¡¯ç¤ºæ­¤å¹«åŠ©

ç¤ºä¾‹:
  bash smartedu_fetch_all.sh -p é«˜ä¸­
  bash smartedu_fetch_all.sh -p é«˜ä¸­ -s è¯­æ–‡,æ•°å­¦ -m "å¿…ä¿® ç¬¬ä¸€å†Œ" -T 3
  bash smartedu_fetch_all.sh -p å°å­¦54 -o ~/Downloads/textbooks
USAGE
  }

  while getopts ":p:s:m:i:o:Rc:d:n:T:h" opt; do
    case "$opt" in
      p) PHASE="$OPTARG" ;;
      s) SUBJECTS="$OPTARG" ;;
      m) MATCH="$OPTARG" ;;
      i) IDS="$OPTARG" ;;
      o) OUT_DIR="$OPTARG" ;;
      R) ONLY_FAILED="1" ;;
      c) HCON="$OPTARG" ;;
      d) DCON="$OPTARG" ;;
      n) LIMIT="$OPTARG" ;;
      T) POST_RETRY="$OPTARG" ;;
      h) usage; exit 0 ;;
      :) echo "éŒ¯èª¤ï¼šé¸é … -$OPTARG éœ€è¦ä¸€å€‹åƒæ•¸ã€‚" >&2; usage; exit 2 ;;
      \?) echo "éŒ¯èª¤ï¼šæœªçŸ¥é¸é … -$OPTARG" >&2; usage; exit 2 ;;
    esac
  done

  # --- åƒæ•¸æ•¸å€¼æ ¡é©—ï¼ˆé¿å…éæ•¸å­—å°è‡´å¾ŒçºŒå ±éŒ¯ï¼‰ ---
  int_re='^[0-9]+$'
  if ! [[ "$HCON" =~ $int_re ]]; then echo "[!] -c å¿…é ˆç‚ºæ•´æ•¸" >&2; exit 2; fi
  if ! [[ "$DCON" =~ $int_re ]]; then echo "[!] -d å¿…é ˆç‚ºæ•´æ•¸" >&2; exit 2; fi
  if [[ -n "$LIMIT" ]] && ! [[ "$LIMIT" =~ $int_re ]]; then echo "[!] -n å¿…é ˆç‚ºæ•´æ•¸" >&2; exit 2; fi
  if ! [[ "$POST_RETRY" =~ $int_re ]]; then echo "[!] -T å¿…é ˆç‚ºæ•´æ•¸" >&2; exit 2; fi

  echo "[*] æº–å‚™ Python ç’°å¢ƒ..."
  if ! command -v python3 >/dev/null 2>&1; then
    if [[ "$(uname)" == "Darwin" ]] && command -v brew >/dev/null 2>&1; then
      echo "[*] ç”¨ Homebrew å®‰è£ python..."; brew install python
    elif [[ "$(uname)" == "Linux" ]] && command -v apt-get >/dev/null 2>&1; then
      echo "[*] ç”¨ apt å®‰è£ python3..."; sudo apt-get update && sudo apt-get install -y python3 python3-venv python3-pip
    else
      echo "[!] æœªæ‰¾åˆ° python3ï¼Œä¸”ç„¡æ³•è‡ªå‹•å®‰è£ã€‚" >&2; exit 1
    fi
  fi

  VENV_DIR="./.venv"
  if [ ! -d "$VENV_DIR" ]; then
    echo "[*] å‰µå»ºè™›æ“¬ç’°å¢ƒ $VENV_DIR"; python3 -m venv "$VENV_DIR"
  fi
  # shellcheck disable=SC1091
  source "$VENV_DIR/bin/activate"
  python3 -m pip install -U pip >/dev/null
  python3 -m pip install -U aiohttp aiofiles tqdm >/dev/null

  export SMARTEDU_PHASE="$PHASE"
  export SMARTEDU_SUBJ="$SUBJECTS"
  export SMARTEDU_MATCH="$MATCH"
  export SMARTEDU_IDS="$IDS"
  export SMARTEDU_OUT_DIR="${OUT_DIR:-./smartedu_textbooks}"
  export SMARTEDU_ONLY_FAILED="$ONLY_FAILED"
  export SMARTEDU_HCON="$HCON"
  export SMARTEDU_DCON="$DCON"
  export SMARTEDU_LIMIT="$LIMIT"
  export SMARTEDU_POST_RETRY="$POST_RETRY"
  export PYTHON_EXEC=1

  echo "[ğŸš€] å•Ÿå‹• Python ä¸‹è¼‰å™¨..."
  TMP_PY="$(mktemp)"
  awk '/^# >>>PYTHON>>>$/{p=1;next} /^# <<<PYTHON<<</{p=0} p' "$0" > "$TMP_PY"
  exec python3 "$TMP_PY"
fi

# >>>PYTHON>>>
# -*- coding: utf-8 -*-
"""
SmartEdu æ‰¹é‡ä¸‹è¼‰å™¨ (polyglot v4.3)
- æ•´è¼ªçµæŸå¾Œè‡ªå‹•é‡è©¦ N è¼ªï¼ˆé è¨­ 2ï¼Œç’°å¢ƒè®Šé‡ SMARTEDU_POST_RETRY æˆ– -T èª¿æ•´ï¼‰ã€‚
"""
from __future__ import annotations

import os, re, json, asyncio, aiohttp, aiofiles, time, logging, traceback
from logging import handlers
from pathlib import Path
from urllib.parse import quote
from typing import List, Dict, Any, Tuple, Optional
from collections import namedtuple
from tqdm import tqdm

# ---------------- è¨­å®š ----------------
Settings = namedtuple("Settings", [
    "PHASE", "SUBJECTS", "MATCH", "IDS", "OUT_DIR", "ONLY_FAILED",
    "HCON", "DCON", "LIMIT", "POST_RETRY"
])

def load_settings_from_env() -> Settings:
    out_env = os.getenv("SMARTEDU_OUT_DIR")
    out_dir = Path(os.path.expanduser(out_env)) if out_env else Path.cwd() / "smartedu_textbooks"
    pr_raw = os.getenv("SMARTEDU_POST_RETRY", "2").strip()
    try:
        pr = max(0, min(5, int(pr_raw)))  # 0..5 åˆç†ç¯„åœ
    except ValueError:
        pr = 2
    return Settings(
        PHASE=os.getenv("SMARTEDU_PHASE", "é«˜ä¸­"),
        SUBJECTS=[s.strip() for s in os.getenv("SMARTEDU_SUBJ", "è¯­æ–‡,æ•°å­¦,è‹±è¯­,æ€æƒ³æ”¿æ²»,å†å²,åœ°ç†,ç‰©ç†,åŒ–å­¦,ç”Ÿç‰©").split(",") if s.strip()],
        MATCH=os.getenv("SMARTEDU_MATCH", "").strip(),
        IDS=[s.strip() for s in os.getenv("SMARTEDU_IDS", "").split(",") if s.strip()],
        OUT_DIR=out_dir,
        ONLY_FAILED=os.getenv("SMARTEDU_ONLY_FAILED", "0") == "1",
        HCON=int(os.getenv("SMARTEDU_HCON", "12")),
        DCON=int(os.getenv("SMARTEDU_DCON", "5")),
        LIMIT=int(raw) if (raw := os.getenv("SMARTEDU_LIMIT", "").strip()).isdigit() else None,
        POST_RETRY=pr,
    )

S_FILE_HOSTS = [
    "https://s-file-1.ykt.cbern.com.cn",
    "https://s-file-2.ykt.cbern.com.cn",
    "https://s-file-3.ykt.cbern.com.cn",
]
R_HOSTS = [
    "https://r1-ndr-oversea.ykt.cbern.com.cn",
    "https://r2-ndr-oversea.ykt.cbern.com.cn",
    "https://r3-ndr-oversea.ykt.cbern.com.cn",
    "https://r1-ndr.ykt.cbern.com.cn",
    "https://r2-ndr.ykt.cbern.com.cn",
    "https://r3-ndr.ykt.cbern.com.cn",
]
ENTRY_PATH = "/zxx/ndrs/resources/tch_material/version/data_version.json"
BASE_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_6_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118 Safari/537.36",
    "Accept": "application/json, text/plain, */*",
    "Accept-Language": "zh-CN,zh;q=0.9",
}
PHASE_TAGS = {
    "å°å­¦": ["å°å­¦"],
    "åˆä¸­": ["åˆä¸­"],
    "é«˜ä¸­": ["é«˜ä¸­", "æ™®é€šé«˜ä¸­"],
    "ç‰¹æ®Šæ•™è‚²": ["ç‰¹æ®Šæ•™è‚²"],
    "å°å­¦54": ["å°å­¦ï¼ˆäº”â€¢å››å­¦åˆ¶ï¼‰", "å°å­¦ï¼ˆäº”Â·å››å­¦åˆ¶ï¼‰"],
    "åˆä¸­54": ["åˆä¸­ï¼ˆäº”â€¢å››å­¦åˆ¶ï¼‰", "åˆä¸­ï¼ˆäº”Â·å››å­¦åˆ¶ï¼‰"],
}

# ---------------- æ—¥èªŒ ----------------
LOGGER = logging.getLogger("smartedu")

def setup_logging(out_dir: Path):
    LOGGER.setLevel(logging.DEBUG)
    LOGGER.handlers.clear()
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(logging.Formatter("[%(asctime)s] %(levelname)s - %(message)s", datefmt="%H:%M:%S"))
    out_dir.mkdir(parents=True, exist_ok=True)
    fh = handlers.RotatingFileHandler(out_dir / "smartedu_download.log", maxBytes=10_000_000, backupCount=2, encoding="utf-8")
    fh.setLevel(logging.DEBUG)
    fh.setFormatter(logging.Formatter("[%(asctime)s] %(levelname)s %(name)s:%(lineno)d - %(message)s", datefmt="%Y-%m-%d %H:%M:%S"))
    LOGGER.addHandler(ch)
    LOGGER.addHandler(fh)

# ---------------- å°å·¥å…· ----------------

def safe_name(s: str) -> str:
    s = re.sub(r'[\\/:*?"<>|]', "_", (s or "").strip())
    return re.sub(r"\s+", " ", s) or "æœªå‘½åæ•™æ"


def book_tags(book: Dict[str, Any]) -> List[str]:
    return [t.get("tag_name", "") for t in (book.get("tag_list") or [])]


def match_phase_subject_keyword(book: Dict[str, Any], settings: Settings) -> bool:
    tags = book_tags(book)
    wants_phase = PHASE_TAGS.get(settings.PHASE, [])
    if wants_phase and not any(any(w in t for w in wants_phase) for t in tags):
        return False
    if settings.SUBJECTS and not any(any(s in t for s in settings.SUBJECTS) for t in tags):
        return False
    if settings.MATCH:
        title = (book.get("title") or (book.get("global_title") or {}).get("zh-CN") or "").lower()
        if not all(k.lower() in title for k in settings.MATCH.split()):
            return False
    return True


async def get_json(session: aiohttp.ClientSession, url: str) -> Optional[Dict | List]:
    for i in range(3):
        try:
            async with session.get(url, headers=BASE_HEADERS, timeout=30) as resp:
                txt = await resp.text()
                if resp.status == 200 and ("json" in (resp.headers.get("Content-Type", "") or "").lower() or txt[:1] in "[{"):
                    return json.loads(txt)
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            LOGGER.debug("JSON è«‹æ±‚å¤±æ•— %s (%d/3): %s", url, i+1, e)
        await asyncio.sleep(1.2 * (i + 1))
    return None


async def get_data_urls(session: aiohttp.ClientSession) -> List[str]:
    for base in S_FILE_HOSTS:
        js = await get_json(session, base + ENTRY_PATH)
        if isinstance(js, dict):
            field = js.get("urls") or js.get("url")
            urls: List[str] = []
            if isinstance(field, str):
                urls = [u.strip() for u in field.split(",") if u.strip()]
            elif isinstance(field, list):
                urls = [str(u).strip() for u in field if str(u).strip()]
            if urls:
                return urls
    return []


async def fetch_all_books(session: aiohttp.ClientSession, urls: List[str]) -> List[Dict[str, Any]]:
    books: List[Dict[str, Any]] = []
    for url in urls:
        js = await get_json(session, url)
        if isinstance(js, list):
            books.extend(js)
    return books


def build_referer(book_id: str) -> str:
    return ("https://basic.smartedu.cn/tchMaterial/detail"
            f"?contentType=assets_document&contentId={book_id}"
            "&catalogType=tchMaterial&subCatalog=tchMaterial")


def derive_filename(item: dict, book_id: str) -> Optional[str]:
    stor = item.get("ti_storage") or (item.get("ti_storages") or [None])[0]
    if isinstance(stor, str) and ".pkg/" in stor:
        tail = stor.replace("cs_path:${ref-path}", "").lstrip("/")
        fname = tail.split(".pkg/", 1)[-1]
        base = fname.split("/")[-1] if fname else None
        if base:
            return base
    fname = item.get("ti_filename")
    if isinstance(fname, str) and fname.lower().endswith(".pdf"):
        return fname.split("/")[-1]
    title = item.get("ti_title") or item.get("title")
    if isinstance(title, str) and title.strip():
        t = title.strip()
        if not t.lower().endswith(".pdf"):
            t += ".pdf"
        return t
    return None


def candidates_from_detail(book_id: str, items: List[dict]) -> List[str]:
    urls = []
    for it in items:
        if (it.get("ti_format") or it.get("format") or "").lower() != "pdf":
            continue
        fname = derive_filename(it, book_id)
        if not fname:
            continue
        raw = f"esp/assets/{book_id}.pkg/{fname}"
        enc = f"esp/assets/{book_id}.pkg/{quote(fname)}"
        for host in R_HOSTS:
            urls.append(f"{host}/edu_product/{raw}")
            urls.append(f"{host}/edu_product/{enc}")
    for host in R_HOSTS:
        urls.append(f"{host}/edu_product/esp/assets/{book_id}.pkg/pdf.pdf")
    # å»é‡
    seen, dedup = set(), []
    for u in urls:
        if u not in seen:
            seen.add(u); dedup.append(u)
    return dedup


async def resolve_candidates(session: aiohttp.ClientSession, book_id: str) -> List[str]:
    for base in S_FILE_HOSTS:
        js = await get_json(session, f"{base}/zxx/ndrv2/resources/tch_material/details/{book_id}.json")
        if isinstance(js, dict):
            items = js.get("ti_items") or []
            if items:
                return candidates_from_detail(book_id, items)
    return [f"{h}/edu_product/esp/assets/{book_id}.pkg/pdf.pdf" for h in R_HOSTS]


async def probe_url_exists(session: aiohttp.ClientSession, url: str, referer: str) -> bool:
    # å„ªå…ˆ HEADï¼›ä¸æ”¯æŒå‰‡ç”¨ Range=0-1 çš„ GETï¼ˆè¿”å› 206 è¦–ç‚ºå­˜åœ¨ï¼‰
    headers = {**BASE_HEADERS, "Referer": referer}
    try:
        async with session.head(url, headers=headers, timeout=20, allow_redirects=True) as r:
            if r.status == 200:
                ct = (r.headers.get("Content-Type", "") or "").lower()
                if "pdf" in ct or url.lower().endswith(".pdf"):
                    cl = r.headers.get("Content-Length")
                    if cl is None or int(cl) > 50 * 1024:
                        return True
    except Exception:
        pass
    # fallback
    try:
        headers["Range"] = "bytes=0-1"
        async with session.get(url, headers=headers, timeout=20, allow_redirects=True) as r:
            if r.status in (200, 206):
                ct = (r.headers.get("Content-Type", "") or "").lower()
                if "pdf" in ct or url.lower().endswith(".pdf"):
                    return True
    except Exception:
        return False
    return False


def is_valid_pdf(path: Path) -> bool:
    try:
        if not path.exists() or path.stat().st_size < 100 * 1024:
            return False
        with open(path, "rb") as f:
            head = f.read(5)
        return head == b"%PDF-"
    except Exception:
        return False


async def download_pdf(session: aiohttp.ClientSession, url: str, dest: Path, book_id: str) -> bool:
    # è‹¥å·²æœ‰å®Œæ•´æ–‡ä»¶ï¼Œè·³é
    if is_valid_pdf(dest):
        LOGGER.info("æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³é: %s", dest.name)
        return True
    tmp = dest.with_suffix(".part")
    start = tmp.stat().st_size if tmp.exists() else 0
    headers = {**BASE_HEADERS, "Referer": build_referer(book_id)}
    if start > 0:
        headers["Range"] = f"bytes={start}-"
    for attempt in range(3):
        try:
            async with session.get(url, headers=headers, timeout=180) as r:
                if r.status not in {200, 206}:
                    LOGGER.debug("ä¸‹è¼‰ HTTP %s: %s", r.status, url)
                    await asyncio.sleep(2.0 * (attempt + 1))
                    continue
                dest.parent.mkdir(parents=True, exist_ok=True)
                mode = "ab" if (start > 0 and r.status == 206) else "wb"
                async with aiofiles.open(tmp, mode) as f:
                    async for chunk in r.content.iter_chunked(1<<14):
                        await f.write(chunk)
                tmp.replace(dest)
                return is_valid_pdf(dest)
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            LOGGER.debug("ä¸‹è¼‰ç•°å¸¸ %s (try %d/3): %s", url, attempt+1, e)
            await asyncio.sleep(2.5 * (attempt + 1))
    LOGGER.warning("ä¸‹è¼‰å¤±æ•—: %s", url)
    return False


async def resolve_one(session: aiohttp.ClientSession, sem: asyncio.Semaphore, book: Dict[str, Any], subjects: List[str]) -> Optional[Tuple[str, str, str, str, Path]]:
    async with sem:
        bid = book.get("id") or book.get("content_id")
        if not bid:
            return None
        title = safe_name(book.get("title") or (book.get("global_title") or {}).get("zh-CN") or bid)
        tags = book_tags(book)
        subj = next((s for s in subjects if any(s in t for t in tags)), "ç¶œåˆ")
        cands = await resolve_candidates(session, bid)
        ref = build_referer(bid)
        for u in cands:
            if await probe_url_exists(session, u, ref):
                return (bid, title, subj, u, Path("/dev/null"))  # ä½”ä½ï¼Œå¯¦éš›è·¯å¾‘ç¨å¾Œç”Ÿæˆ
        return None


async def run_normal_mode(session: aiohttp.ClientSession, settings: Settings):
    # æ§‹å»ºæ›¸ç›®
    if settings.IDS:
        books = [{"id": i, "title": i, "tag_list": [{"tag_name": settings.PHASE}]} for i in settings.IDS]
    else:
        LOGGER.info("ğŸ” è®€å–é ç¨‹æ•¸æ“šç´¢å¼•...")
        data_urls = await get_data_urls(session)
        if not data_urls:
            LOGGER.error("ç„¡æ³•ç²å–æ•¸æ“šç´¢å¼• URL åˆ—è¡¨ï¼"); return
        all_books = await fetch_all_books(session, data_urls)
        LOGGER.info("ğŸ“š ç¸½æ›¸ç›®æ•¸: %d", len(all_books))
        books = [b for b in all_books if match_phase_subject_keyword(b, settings)]
    if settings.LIMIT is not None:
        books = books[:settings.LIMIT]
    LOGGER.info("ğŸ¯ ç¯©é¸å¾Œï¼Œæº–å‚™è™•ç† %d æœ¬æ›¸ã€‚", len(books))
    if not books:
        return

    # è§£æ
    sem_head = asyncio.Semaphore(settings.HCON)
    resolve_tasks = [asyncio.create_task(resolve_one(session, sem_head, b, settings.SUBJECTS)) for b in books]
    resolved_raw: List[Tuple[str, str, str, str, Path]] = []
    for fut in tqdm(asyncio.as_completed(resolve_tasks), total=len(resolve_tasks), desc="è§£æç›´éˆ", ncols=100):
        r = await fut
        if r: resolved_raw.append(r)
    LOGGER.info("ğŸ”— è§£æå®Œæˆ: %d æœ¬å¯ä¸‹è¼‰", len(resolved_raw))

    # å¡«å¯«å¯¦éš›ä¿å­˜è·¯å¾‘ä¸¦å»é‡ï¼ˆæŒ‰ç›®æ¨™è·¯å¾‘ï¼‰
    resolved: List[Tuple[str, str, str, str, Path]] = []
    seen_paths = set()
    for bid, title, subj, url, _ in resolved_raw:
        dest = settings.OUT_DIR / subj / f"{title}.pdf"
        if dest in seen_paths:
            continue
        seen_paths.add(dest)
        resolved.append((bid, title, subj, url, dest))

    # ä¸‹è¼‰ï¼ˆé¦–è¼ªï¼‰
    down_tasks = [asyncio.create_task(download_pdf(session, url, dest, bid)) for (bid, title, subj, url, dest) in resolved]
    success, failed = 0, 0
    index_success, failed_list = [], []
    for fut, meta in zip(tqdm(asyncio.as_completed(down_tasks), total=len(down_tasks), desc="PDF ä¸‹è¼‰", ncols=100), resolved):
        ok = await fut
        bid, title, subj, url, dest = meta
        if ok:
            success += 1
            index_success.append({"id": bid, "title": title, "subject": subj, "pdf_url": url, "path": str(dest)})
        else:
            failed += 1
            failed_list.append({"id": bid, "title": title, "subject": subj, "url": url, "path": str(dest)})

    # è‡ªå‹•é‡è©¦ï¼ˆåƒ…é‡å°å¤±æ•—é …ï¼‰
    rounds = settings.POST_RETRY
    for round_idx in range(1, rounds + 1):
        if not failed_list:
            break
        LOGGER.info("ğŸ” è‡ªå‹•é‡è©¦ ç¬¬ %d/%d è¼ªï¼šå¾…é‡è©¦ %d æœ¬", round_idx, rounds, len(failed_list))
        tasks = [asyncio.create_task(download_pdf(session, it["url"], Path(it["path"]), it["id"])) for it in failed_list]
        new_failed = []
        for fut, it in zip(tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=f"é‡è©¦è¼ª {round_idx}", ncols=100), list(failed_list)):
            if await fut:
                success += 1
                index_success.append({"id": it["id"], "title": it["title"], "subject": it["subject"], "pdf_url": it["url"], "path": it["path"]})
            else:
                new_failed.append(it)
        failed = len(new_failed)
        failed_list = new_failed
        if new_failed:
            await asyncio.sleep(1.0 * round_idx)  # è¼•å¾®é€€é¿

    # è¼¸å‡º
    async with aiofiles.open(settings.OUT_DIR / "index.json", "w", encoding="utf-8") as f:
        await f.write(json.dumps(index_success, ensure_ascii=False, indent=2))
    async with aiofiles.open(settings.OUT_DIR / "failed.json", "w", encoding="utf-8") as f:
        await f.write(json.dumps(failed_list, ensure_ascii=False, indent=2))

    retried = settings.POST_RETRY if settings.POST_RETRY else 0
    LOGGER.info("âœ… ç¸½çµï¼šæˆåŠŸ %dï¼Œä»å¤±æ•— %dï¼ˆè‡ªå‹•é‡è©¦è¼ªæ•¸ %dï¼‰", success, len(failed_list), retried)
    LOGGER.info("ğŸ“„ ç´¢å¼•: %s", settings.OUT_DIR / "index.json")
    if failed_list:
        LOGGER.warning("âš ï¸ ä»æœ‰ %d æœ¬ä¸‹è¼‰å¤±æ•—ã€‚å¯å†æ¬¡é‹è¡Œæœ¬è…³æœ¬ï¼ˆå°‡åƒ…çºŒå‚³æœªå®Œæˆéƒ¨åˆ†ï¼‰ï¼Œæˆ–ä½¿ç”¨ -R åƒ…é‡è©¦å¤±æ•—æ¸…å–®ã€‚", len(failed_list))


async def run_retry_mode(session: aiohttp.ClientSession, settings: Settings):
    failed_path = settings.OUT_DIR / "failed.json"
    if not failed_path.exists():
        LOGGER.error("é‡è©¦æ¨¡å¼å¤±æ•—ï¼šæœªæ‰¾åˆ° %s", failed_path); return
    async with aiofiles.open(failed_path, "r", encoding="utf-8") as f:
        items = json.loads(await f.read())
    if not items:
        LOGGER.info("ğŸ‰ å¤±æ•—è¨˜éŒ„ç‚ºç©ºï¼Œç„¡éœ€é‡è©¦ã€‚"); return
    tasks = [asyncio.create_task(download_pdf(session, it["url"], Path(it["path"]), it["id"])) for it in items]
    new_failed = []
    ok = 0
    for fut, it in zip(tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="é‡è©¦ä¸‹è¼‰", ncols=100), items):
        if await fut:
            ok += 1
        else:
            new_failed.append(it)
    async with aiofiles.open(failed_path, "w", encoding="utf-8") as f:
        await f.write(json.dumps(new_failed, ensure_ascii=False, indent=2))
    LOGGER.info("âœ… é‡è©¦å®Œæˆï¼šæˆåŠŸ %dï¼Œä»å¤±æ•— %d", ok, len(new_failed))


async def main():
    settings = load_settings_from_env()
    setup_logging(settings.OUT_DIR)
    LOGGER.info("ğŸ›ï¸ éšæ®µ:%s ç§‘ç›®:%s é—œéµè©:%s IDS:%d è‡ªå‹•é‡è©¦:%d", settings.PHASE, ",".join(settings.SUBJECTS) or "å…¨éƒ¨", settings.MATCH or "ç„¡", len(settings.IDS), settings.POST_RETRY)
    LOGGER.info("ğŸ“ ä¸‹è¼‰ç›®éŒ„: %s", settings.OUT_DIR)
    t0 = time.time()
    async with aiohttp.ClientSession(headers=BASE_HEADERS) as session:
        if settings.ONLY_FAILED:
            await run_retry_mode(session, settings)
        else:
            await run_normal_mode(session, settings)
    LOGGER.info("â±ï¸ ç¸½è€—æ™‚: %.2f ç§’", time.time() - t0)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâ›” å·²æ‰‹å‹•ä¸­æ–·")
    except Exception:
        LOGGER.critical("è‡´å‘½éŒ¯èª¤:\n%s", traceback.format_exc())
# <<<PYTHON<<<